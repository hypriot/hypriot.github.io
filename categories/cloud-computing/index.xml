<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cloud Computing on Docker Pirates ARMed with explosive stuff</title>
    <link>https://blog.hypriot.com/categories/cloud-computing/index.xml</link>
    <description>Recent content in Cloud Computing on Docker Pirates ARMed with explosive stuff</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://blog.hypriot.com/categories/cloud-computing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Setup Kubernetes on a Raspberry Pi Cluster easily the official way!</title>
      <link>https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/</link>
      <pubDate>Wed, 11 Jan 2017 14:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; shares the pole position with Docker in the category &amp;ldquo;orchestration solutions for Raspberry Pi cluster&amp;rdquo;.
However it&amp;rsquo;s setup process has been elaborate – until &lt;a href=&#34;http://blog.kubernetes.io/2016/09/how-we-made-kubernetes-easy-to-install.html&#34;&gt;v1.4 with the kubeadm announcement&lt;/a&gt;.
With that effort, Kubernetes changed this game completely and can be up and running officially within no time.&lt;/p&gt;

&lt;p&gt;I am very happy to announce that this blog post has been written in collaboration with &lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt;, an independent maintainer of Kubernetes (his story is very interesting,  you can read it in a &lt;a href=&#34;https://www.cncf.io/blog/2016/11/29/diversity-scholarship-series-programming-journey-becoming-kubernetes-maintainer&#34;&gt;CNCF blogpost&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/raspberry-pi-cluster.png&#34; alt=&#34;SwarmClusterHA&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-kubernetes&#34;&gt;Why Kubernetes?&lt;/h2&gt;

&lt;p&gt;As shown in my recent &lt;a href=&#34;http://www.slideshare.net/MathiasRenner/high-availability-performance-of-kubernetes-and-docker-swarm-on-a-raspberry-pi-cluster/&#34;&gt;talk&lt;/a&gt;, there are many software suites available to manage a cluster of computers. There is Kubernetes, Docker Swarm, Mesos, OpenStack, Hadoop YARN, Nomad&amp;hellip; just to name a few.&lt;/p&gt;

&lt;p&gt;However, at Hypriot we have always been in love with tiny devices. So when working with an orchestrator, the maximum power we wanna use is what&amp;rsquo;s provided by a Raspberry Pi. Why? We have IoT networks in mind that will hold a large share in tomorrow&amp;rsquo;s IT infrastructure. At their edges, the power required for large orchestrators simply is not available.&lt;/p&gt;

&lt;p&gt;This boundary of resources leads to several requirements that need to be checked before we start getting our hands dirty with an orchestrator:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight:&lt;/strong&gt; Software should be fit on a Raspberry Pi or smaller. As proofed in my talk mentioned above, Kubernetes painlessly runs on a Raspberry Pi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ARM compatible:&lt;/strong&gt; Since the ARM CPU architecture is designed for low energy consumption but still able to deliver a decent portion of power, the Raspberry Pi runs an ARM CPU. Thanks to Lucas, Kubernetes is ARM compatible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General purpose:&lt;/strong&gt; Hadoop or Apache Spark are great for data analysis. But what if your use case changes? We prefer general purpose software that allows to run &lt;strong&gt;anything&lt;/strong&gt;. Kubernetes uses a container runtime (with Docker as the 100% supported runtime for the time being) that allows to run whatever you want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Production ready:&lt;/strong&gt; Since we compare Kubernetes against a production ready Docker suite, let&amp;rsquo;s be fair and only choose equivalents. Kubernetes itself is production ready, and while the ARM port has some small issues, it&amp;rsquo;s working exactly as expected when going the official kubeadm route, which also will mature with time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, Kubernetes seems to be a compelling competitor to Docker Swarm. Let&amp;rsquo;s get our hands on it!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;wait-what-about-kubernetes-on-arm&#34;&gt;Wait – what about &lt;em&gt;Kubernetes-on-ARM&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;If you followed the discussion of Kubernetes on ARM for some time, you probably know about Lucas&amp;rsquo; project &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;kubernetes-on-ARM&lt;/a&gt;. Since the beginning of the movement to bring Kubernetes on ARM in 2015, this project has always been the most stable and updated.&lt;/p&gt;

&lt;p&gt;However, during 2016, Lucas&amp;rsquo; contributions have successfully been merged into official Kubernetes repositories, such that there is no point any more for using the kubernetes-on-ARM project.
In fact, the features of that project are far behind of what&amp;rsquo;s now implemented in the official repos, &lt;strong&gt;and that has been Lucas&amp;rsquo; goal from the beginning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re up to using Kubernetes, please stick to the official repos now. And as of the kubeadm documentation, &lt;strong&gt;the following setup is considered official for Kubernetes on ARM.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;at-first-flash-hypriotos-on-your-sd-cards&#34;&gt;At first: Flash HypriotOS on your SD cards&lt;/h2&gt;

&lt;p&gt;As hardware, take at least two Raspberry Pis and make sure they are connected to each other and to the Internet.&lt;/p&gt;

&lt;p&gt;First, we need an operating system. Download and flash &lt;a href=&#34;https://github.com/hypriot/image-builder-rpi/releases&#34;&gt;HypriotOS&lt;/a&gt;. The fastest way to download and flash HypriotOS on your SD cards is by using our &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;flash tool&lt;/a&gt; like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flash --hostname node01 https://github.com/hypriot/image-builder-rpi/releases/download/v1.4.0/hypriotos-rpi-v1.4.0.img.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Provision all Raspberry Pis you have like this and boot them up.&lt;/p&gt;

&lt;p&gt;Afterwards, SSH into the Raspberry Pis with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh pirate@node01.local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The password &lt;code&gt;hypriot&lt;/code&gt; will grant you access.&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;install-kubernetes&#34;&gt;Install Kubernetes&lt;/h2&gt;

&lt;p&gt;The installation requires root privileges. Retrieve them by&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo su -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To install Kubernetes and its dependencies, only some commands are required.
First, trust the kubernetes APT key and add the official APT Kubernetes repository on every node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
$ echo &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot; &amp;gt; /etc/apt/sources.list.d/kubernetes.list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and then just install &lt;code&gt;kubeadm&lt;/code&gt; on every node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ apt-get update &amp;amp;&amp;amp; apt-get install -y kubeadm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the previous command has been finished, initialize Kubernetes on the &lt;strong&gt;master node&lt;/strong&gt; with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm init --pod-network-cidr 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is important that you add the &lt;code&gt;--pod-network-cidr&lt;/code&gt; command as given here, because we will use &lt;a href=&#34;https://coreos.com/flannel/docs/latest/&#34;&gt;flannel&lt;/a&gt;. Read the next notes about flannel if you wanna know why.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Some notes about flannel&lt;/strong&gt;: We picked flannel here because that’s the only available solution for ARM at the moment (this is subject to change in the future though).&lt;/p&gt;

&lt;p&gt;flannel can use and is using in this example the Kubernetes API to store metadata about the Pod CIDR allocations, and therefore we need to tell &lt;em&gt;Kubernetes&lt;/em&gt; first which subnet we want to use. The subnet we chose here is somehow fixed, because the &lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml#L25&#34;&gt;flannel configuration file&lt;/a&gt; that we&amp;rsquo;ll use later in this guide predefines the equivalent subnet. Of course, you can adapt both.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you are connected via WIFI instead of Ethernet, add &lt;code&gt;--apiserver-advertise-address=&amp;lt;wifi-ip-address&amp;gt;&lt;/code&gt; as parameter to &lt;code&gt;kubeadm init&lt;/code&gt; in order to publish Kubernetes&amp;rsquo; API via WiFi. Feel free to explore the other options that exist for &lt;code&gt;kubeadm init&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After Kubernetes has been initialized, the last lines of your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/init.png&#34; alt=&#34;init&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To start using your cluster, you need to run (as a regular user):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo cp /etc/kubernetes/admin.conf $HOME/
$ sudo chown $(id -u):$(id -g) $HOME/admin.conf
$ export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, as told by that output, let all other nodes join the cluster via the given &lt;code&gt;kubeadm join&lt;/code&gt; command. It will look something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm join --token=bb14ca.e8bbbedf40c58788 192.168.0.34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After some seconds, you should see all nodes in your cluster when executing the following &lt;strong&gt;on the master node&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/get-nodes.png&#34; alt=&#34;k8S&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, &lt;strong&gt;we need to setup flannel v0.9.1 as the Pod network driver&lt;/strong&gt;. Do not use &lt;a href=&#34;https://github.com/coreos/flannel/releases/tag/v0.8.0&#34;&gt;v0.8.0&lt;/a&gt; due to a known &lt;a href=&#34;https://github.com/coreos/flannel/issues/773&#34;&gt;bug&lt;/a&gt; that can cause a &lt;code&gt;CrashLoopBackOff&lt;/code&gt; error. Run this &lt;strong&gt;on the master node&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -sSL https://rawgit.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml | sed &amp;quot;s/amd64/arm/g&amp;quot; | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/flannel.png&#34; alt=&#34;k8S&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then wait until all flannel and all other cluster-internal Pods are &lt;code&gt;Running&lt;/code&gt; before you continue:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get po --all-namespaces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nice, it seems like they are all &lt;code&gt;Running&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/show-namespaces.png&#34; alt=&#34;show-namespaces&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for the setup of Kubernetes! Next, let&amp;rsquo;s actually spin up a service on the cluster!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;test-your-setup-with-a-tiny-service&#34;&gt;Test your setup with a tiny service&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start a simple service so see if the cluster actually can publish a service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run hypriot --image=hypriot/rpi-busybox-httpd --replicas=3 --port=80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command starts set of containers called &lt;strong&gt;hypriot&lt;/strong&gt; from the image &lt;strong&gt;hypriot/rpi-busybox-httpd&lt;/strong&gt; and defines the port the container listens on at &lt;strong&gt;80&lt;/strong&gt;. The service will be &lt;strong&gt;replicated with 3 containers&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Next, expose the Pods in the above created Deployment in a Service with a stable name and IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment hypriot --port 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Now, let&amp;rsquo;s check if all three desired containers are up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get endpoints hypriot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see three endpoints (= containers) like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/show-endpoints.png&#34; alt=&#34;show-endpoints&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s curl one of them to see if the service is up:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/curl-service.png&#34; alt=&#34;curl-service&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The HTML is the response of the service. Good, it&amp;rsquo;s up and running! Next, let&amp;rsquo;s see how we can access it from outside the cluster!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;finally-access-your-service-from-outside-the-cluster&#34;&gt;Finally access your service from outside the cluster&lt;/h2&gt;

&lt;p&gt;We will now deploy an example Ingress Controller to manage incoming requests from the outside world onto our tiny service. Also, in this example we we&amp;rsquo;ll use &lt;a href=&#34;https://traefik.io&#34;&gt;Traefik&lt;/a&gt; as load balancer. Read the following notes if you wanna know more about Ingress and Traefik.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In contrast to Docker Swarm, Kubernetes itself does not provide an option to define a specific port that you can use to access a service&lt;/strong&gt;. According to Lucas this is an important design decision; routing of incoming requests should be handled by a third party, such as a load balancer or a webserver, but not by the core product. The core Kubernetes should be lean and extensible, and encourage others to build tools on top of it for their specific needs.&lt;/p&gt;

&lt;p&gt;Regarding load balancers in front of a cluster, there is &lt;a href=&#34;http://kubernetes.io/docs/user-guide/ingress/&#34;&gt;the Ingress API object&lt;/a&gt; and some sample &lt;a href=&#34;https://github.com/kubernetes/ingress&#34;&gt;Ingress Controllers&lt;/a&gt;. Ingress is a built-in way of exposing Services to the outside world via an Ingress Controller that anyone can build. An &lt;em&gt;Ingress rule&lt;/em&gt; defines how traffic should flow from the node the Ingress controller runs on to services inside of the cluster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, let&amp;rsquo;s deploy traefik as load balancer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://raw.githubusercontent.com/hypriot/rpi-traefik/master/traefik-k8s-example.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Label the node you want to be the load balancer. Then the Traefik Ingress Controller will land on the node you specified. Run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label node &amp;lt;load balancer-node&amp;gt; nginx-controller=traefik
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, create an Ingress object that makes Traefik load balance traffic on port &lt;code&gt;80&lt;/code&gt; to the &lt;code&gt;hypriot&lt;/code&gt; service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; hypriot-ingress.yaml &amp;lt;&amp;lt;EOF
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hypriot
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: hypriot
          servicePort: 80
EOF
$ kubectl apply -f hypriot-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visit the loadbalancing node&amp;rsquo;s IP address in your browser and you should see a nice web page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/hypriot-website.png&#34; alt=&#34;curl-service&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t see a website there yet, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and make sure all hypriot Pods are in the &lt;code&gt;Running&lt;/code&gt; state.&lt;/p&gt;

&lt;p&gt;Wait until you see that all Pods are running, and a nice Hypriot website should appear!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;tear-down-the-cluster&#34;&gt;Tear down the cluster&lt;/h2&gt;

&lt;p&gt;If you wanna reset the whole cluster to the state after a fresh install, just run this on each node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm reset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition, it is recommended to delete some additional files &lt;a href=&#34;http://stackoverflow.com/questions/41359224/kubernetes-failed-to-setup-network-for-pod-after-executed-kubeadm-reset/41372829#41372829&#34;&gt;as it is mentioned here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;optional-deploy-the-kubernetes-dashboard&#34;&gt;Optional: Deploy the Kubernetes dashboard&lt;/h2&gt;

&lt;p&gt;The dashboard is a wonderful interface to visualize the state of the cluster. Start it with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard-arm.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the kubernetes-dashboard service to use &lt;code&gt;type: ClusterIP&lt;/code&gt; to &lt;code&gt;type: NodePort&lt;/code&gt;, see &lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above&#34;&gt;Accessing Kubernetes Dashboard&lt;/a&gt; for more details.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n kube-system edit service kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command provides the port that the dashboard is exposed at on every node with the NodePort function of Services, which is another way to expose your Services to the outside of your cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n kube-system get service kubernetes-dashboard -o template --template=&amp;quot;{{ (index .spec.ports 0).nodePort }}&amp;quot; | xargs echo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can checkout the dashboard on any node&amp;rsquo;s IP address on that port! Make sure to use &lt;code&gt;https&lt;/code&gt; when accessing the dashboard, for example if running on port &lt;code&gt;31657&lt;/code&gt; access it at &lt;code&gt;https://node:31657&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newer versions of the Kubernetes Dashboard require either a &lt;code&gt;Kubeconfig&lt;/code&gt; or &lt;code&gt;Token&lt;/code&gt; to view information on the dashboard. &lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Access-control#introduction&#34;&gt;Bearer tokens&lt;/a&gt; are recommended to setup proper permissions for a user, but to test the &lt;code&gt;replicaset-controller-token&lt;/code&gt; Token may be used to test.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n kube-system describe secret `kubectl -n kube-system get secret | grep replicaset-controller-token | awk &#39;{print $1}&#39;` | grep token: | awk &#39;{print $2}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;you-like-a-follow-up&#34;&gt;You like a follow-up?&lt;/h2&gt;

&lt;p&gt;It was our goal to show that Kubernetes indeed works well on ARM (and ARM 64-bit!). For more examples including the AMD64 platform, check out the &lt;a href=&#34;http://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;official kubeadm documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We might follow-up this blog post with a more in-depth post about the current and planned state of Kubernetes officially on ARM and more, so stay tuned and tell Lucas if that&amp;rsquo;s something you&amp;rsquo;re interested in reading.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;Mathias Renner&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/kubernetesonarm&#34;&gt;Lucas Käldström&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing High Availability of Docker Swarm on a Raspberry Pi Cluster (Updated)</title>
      <link>https://blog.hypriot.com/post/high-availability-with-docker/</link>
      <pubDate>Wed, 26 Oct 2016 18:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/high-availability-with-docker/</guid>
      <description>&lt;p&gt;In its release in June this year, Docker announced two exciting news about the Docker Engine: First, the Engine 1.12 comes with built-in high availability features, called &amp;ldquo;Docker Swarm Mode&amp;rdquo;. And second, Docker started providing official support for the ARM architecture.&lt;/p&gt;

&lt;p&gt;These two news combined beg for testing the new capabilities in reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/high-availability-docker-swarm.png&#34; alt=&#34;SwarmClusterHA&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;old&amp;rdquo; Docker Swarm already offered some high availability features, but the new called &amp;ldquo;Swarm Mode&amp;rdquo; goes far beyond: To me, the most important capabilities to mention are: built-in security, rolling updates and - maybe one of the most important benefits - super user-friendliness. Production ready clustering based on powerful container technology has probably never been easier before.&lt;/p&gt;

&lt;p&gt;Also, the release of Docker 1.12 comes with official support for the ARM architecture. Running Docker on ARM has been possible for long time, but now, users can install it the same way on their Raspberry Pis as they do it on any other architecture: Using Docker&amp;rsquo;s official repositories. If you are one of our regular readers, you know that our team at Hypriot intensively collaborated with Docker to make this official support possible. So we are also very happy that this huge milestone that many people asked for is eventually checked!&lt;/p&gt;

&lt;p&gt;Now, with these two news, let&amp;rsquo;s get our hands dirty and test if the announced promises hold.&lt;/p&gt;

&lt;h2 id=&#34;setup-the-cluster&#34;&gt;Setup the cluster&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;As hardware&lt;/strong&gt;, I have 5x Raspberry Pi 3, connected in a network with Internet access.&lt;/p&gt;

&lt;p&gt;The Raspberry Pis with their SD cards, the network switch, power input and all cables live in a &lt;a href=&#34;https://www.picocluster.com/collections/starter-picocluster-kits/products/pico-5-raspberry-pi-starter-kit?variant=29344698892&#34;&gt;hardware kit from PicoCluster&lt;/a&gt;. Thanks to PicoCluster for providing the kit to us for testing!&lt;/p&gt;

&lt;p&gt;See how the cluster looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/PicoCluster.jpg&#34; alt=&#34;PicoClusterBuilt&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As software&lt;/strong&gt;, I chose HypriotOS. I also tried Raspbian, but it requires a large system upgrade after the first boot in order to provide all features required by Docker (e.g. VXLAN kernel module). Before and after the system upgrade, I also had to run &lt;a href=&#34;https://github.com/docker/docker/blob/master/contrib/check-config.sh&#34;&gt;Docker&amp;rsquo;s test script&lt;/a&gt; to make sure everything is ok. All of this is not a big deal, but if there is an OS available that comes out of the box with all that Docker needs (and even adds lots of useful optimizations), it simply saves time and provides a hassle-free experience. So I flashed all SD cards with HypriotOS using our &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;flash tool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To connect to the cluster&lt;/strong&gt;, I SSHed into all the nodes via &lt;a href=&#34;http://gnometerminator.blogspot.de/p/introduction.html&#34;&gt;Terminator&lt;/a&gt;. Terminator is my preferred tool to organize multiple terminals in a single window. It allows to send commands to several terminals at once. Soon, I&amp;rsquo;ll have a look at tmux and might switch, but for now I recommend Terminator :)&lt;/p&gt;

&lt;p&gt;Finally, we have 5 terminals opened, all waiting for commands as you see in the following image. Please don&amp;rsquo;t care about the commands running in each terminal for now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/terminal.png&#34; alt=&#34;Terminator&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all to do before getting our hands on Docker itself!&lt;/p&gt;

&lt;h2 id=&#34;setup-docker-swarm-on-the-cluster&#34;&gt;Setup Docker Swarm on the cluster&lt;/h2&gt;

&lt;p&gt;On HypriotOS, you&amp;rsquo;ll have the latest Docker installed, so we can instantly start playing.&lt;/p&gt;

&lt;p&gt;On an arbitrary node of the cluster, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker swarm init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should look similar to that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Swarm initialized: current node (node-master) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-1acm9qa7b0hmzz5v8t40d75v5fsgckeu2z5ds6ls0x7cny7l8p-307wqrc8756akpxjxls9abbbs \
    [2a02:810d:8600:2a78:f6c6:d67d:6912:17ca]:2377

To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As explained in this output, now you need to run the presented &lt;code&gt;docker swarm join ...&lt;/code&gt; command on all other nodes, which you want to join the cluster.&lt;/p&gt;

&lt;p&gt;Finally, on the node at which you executed &lt;code&gt;docker swarm init&lt;/code&gt;, check if all nodes of your cluster successfully formed a swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker node ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the terminal prints a list of all nodes of your cluster, the setup is all done! Forming a cluster cannot be easier, can it?&lt;/p&gt;

&lt;h2 id=&#34;execution-of-tests&#34;&gt;Execution of tests&lt;/h2&gt;

&lt;p&gt;My tests of Docker&amp;rsquo;s ability to recover from failures comprise the following use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ethernet interface of a random node (master or slave) got unavailable&lt;/li&gt;
&lt;li&gt;A random node completely rebooted&lt;/li&gt;
&lt;li&gt;A random node instantly crashed (unplug power source)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I documented one of the test runs in the following screencast.&lt;/p&gt;

&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/185361173&#34; width=&#34;640&#34; height=&#34;360&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/185361173&#34;&gt;Testing High Availability Docker Swarm Mode&lt;/a&gt; from &lt;a href=&#34;https://vimeo.com/user54109827&#34;&gt;Mathias Renner&lt;/a&gt; on &lt;a href=&#34;https://vimeo.com&#34;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;As shown in the screencast, Docker is able to recover from failure of the ethernet interface. After testing the other use cases later on, Docker recovered flawlessly from a reboot and crash as well. Note that this also holds for master nodes, not only for slave nodes! This is remarkable because in contrast to slave nodes, master nodes hold important data of the cluster state. A master nodes&amp;rsquo;s outage is critical for the health of the cluster. With e.g. &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-on-arm&lt;/a&gt;, the cluster does not recover after rebooting or crashing a master node.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Edit on 6.11.2016&lt;/strong&gt;: Jérémy Derussé and Nikolay Kushin reported in the discussion below that for them a reboot or crash of a node did not result in a healthy cluster state after bringing up the node again. As of today, unfortunately I can confirm this, with Docker version 1.12.1, 1.12.2 as well as 1.12.3. I cannot explain why during my tests for this post a crashed node recovered smoothly.&lt;/p&gt;

&lt;p&gt;Based on Jeremy&amp;rsquo;s report, I was able to create a quick fix for this issue. On a node, simply run:&lt;/p&gt;

&lt;p&gt;&amp;ldquo;sudo crontab -e&amp;rdquo;&lt;/p&gt;

&lt;p&gt;There, insert the following line (without quotation marks) and save the file.&lt;/p&gt;

&lt;p&gt;&amp;rdquo;@reboot docker ps&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Having this configured, after a crash or reboot the node recovered correctly in my tests. I look forward for feedback about this issue! Please use the discussion below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;more-to-come-soon&#34;&gt;More to come soon&lt;/h2&gt;

&lt;p&gt;This post is only a small chunk of the data I gathered during the tests. The next option to get the details is during my talk at the &lt;a href=&#34;http://highload.co/&#34;&gt;HighLoad++ Conference&lt;/a&gt; in Moskow, Russia. I&amp;rsquo;d be happy if you can make it there!&lt;/p&gt;

&lt;p&gt;Also, this is not the end of the story of course. Thorough testing requires also measuring incoming requests from an external load tester to the cluster while a failure occurs. After some research, I have not found any evidence that someone has ever performed that tests (please correct me if I&amp;rsquo;m wrong!).&lt;/p&gt;

&lt;p&gt;Moreover, Kubernetes received much attention lately since its support for ARM became better and better. So wouldn&amp;rsquo;t it be interesting to see if Docker or Kubernetes is better in keeping a service available and performing well, even if there are outages in the cluster?&lt;/p&gt;

&lt;p&gt;So there&amp;rsquo;s more coming soon, just stay tuned for the upcoming posts :)&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;@MathiasRenner&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning about Cloud Computing with Hypriot Cluster Lab at ICT 2016</title>
      <link>https://blog.hypriot.com/post/talk_ict_2016_international_conference_telecommunications/</link>
      <pubDate>Mon, 06 Jun 2016 18:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/talk_ict_2016_international_conference_telecommunications/</guid>
      <description>&lt;p&gt;Our cooperation with University of Bamberg pays off once again: We had the chance to present our Hypriot Cluster Lab at &lt;a href=&#34;http://ict-2016.org/&#34;&gt;ICT 2016&lt;/a&gt;, the International Conference on Telecommunications. There, we were able to show the Hypriot Cluster Lab to the attendees, who were mainly scientific researchers from all over the globe.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/ict_logo.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;We built the Hypriot Cluster Lab as lab environment that allows to easily teach about clustering and cloud computing. At the conference, the Hypriot Cluster Lab piqued lots of interest, so we are confident that it will be used more and more in educational institutions.&lt;/p&gt;

&lt;p&gt;Now, after the show, we are publishing &lt;a href=&#34;http://ict-2016.org/pdf/Demo1.pdf&#34;&gt;the paper&lt;/a&gt; that has been accepted at the conference and the &lt;a href=&#34;https://blog.hypriot.com/images/ict-2016-greece/ICT-Presentation.pdf&#34;&gt;presentation slides&lt;/a&gt;. However, the paper is (intentionally) very brief and the slides are not throughout self-explanatory, so we&amp;rsquo;ve also written down what we&amp;rsquo;ve presented to the audience back in Greece.&lt;/p&gt;

&lt;h2 id=&#34;iot-requires-expansion-to-small&#34;&gt;IoT requires &amp;ldquo;Expansion to small&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Building cloud computing infrastructures with big servers is well understood. Even large cloud infrastructures powered by container technology are commonly in use. One of the pioneers to mention here is Google with its container-based cloud platform &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, the uprising Internet of Things (IoT) will not allow to equip the edge of the network such as sensor networks in smart home/grid/production environments with big servers only. There, you especially need small devices with less computational power and energy demand. These devices require to be small and cheap enough to install up to several thousands of them in a smart environment, thereby creating sensor networks. For example think of sensor networks on wind farms:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/wind.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;
&lt;div style=&#34;padding-left:34.4em; font-size: smaller&#34;&gt;Image courtesy of &lt;a href=&#34;https://www.flickr.com/photos/ewea/16577911487/in/photolist-rfW8w8-ipD2tM-ipC49q-7d8FqR-ipCNzR-ipC1dU-ipBRBC-ipCGWN-ipCkFm-njeuG6-ipCC31-ipCnPv-ayYLge-ipCJ61-ayYMmc-ipBGVN-ayYHLH-9mZuNi-ayYMzX-oWbzp6-ayYK6n-7ZaHBQ-q8T7wm-az2p4W-8LowiB-az2o8s-ayYJJt-az2p2f-3euhCb-ayYMWD-ayYNgZ-ayYL48-az2riu-6YiWvR-az2qkC-ayYMev-az2rxU-az2on1-ayYK2V-ayYHrz-az2oru-az2ov7-az2oeW-qQMUJN-auVRqy-az2oDm-ayYM8Z-i5kW85-ayYJTV-az2rCE&#34;&gt;European Wind Energy Association&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;At this point, Hypriot Cluster Lab (HCL) comes into play:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;HCL is a proof of concept that building clusters using container technology (in our case: Docker) also works well on small devices, not only on big servers.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This &amp;ldquo;Expansion to Small&amp;rdquo; was the theme of the ICT 2016, which made HCL a perfect fit to the conference&amp;rsquo;s agenda.&lt;/p&gt;

&lt;h2 id=&#34;hcl-makes-a-tangible-iot-like-cluster-affordable&#34;&gt;HCL makes a tangible IoT-like cluster affordable&lt;/h2&gt;

&lt;p&gt;HCL is not only a proof of concept that clustering with container technology on IoT devices works, it also serves a second purpose: It is cheap enough to be used as a learning platform to play with clustering or cloud computing on your desk. The minimal hardware configuration requires three Raspberry Pis plus accessoires, which is affordable for everyone to get their hands on a real, tangible cluster. We&amp;rsquo;ve already seen lecturers, students and pupils getting started on the HCL with different topics related to cloud computing, e.g. playing with micro services or load balancing. Some use cases can be found &lt;a href=&#34;https://github.com/hypriot/rpi-cluster-lab-demos&#34;&gt;in one of our repos here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even though the HCL is intended to be used as a tool to learn, its containing concepts are applicable for promising IoT software solutions. Comparing IoT&amp;rsquo;s general requirements for hard- and software that I identified in my bachelor thesis (&lt;a href=&#34;https://medium.com/@mathiasrenner/docker-container-virtualization-and-the-internet-of-things-bachelor-thesis-a6bc783b81fa#.f09czsq2e&#34;&gt;details here&lt;/a&gt;) show that many of those aspects are already visible in the HCL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/iot-requirements.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;All in all, the conference was an enriching experience. Discussing with scientific researches and professors from many different countries resulted in high-grade feedback and inspirations of how we can further improve the HCL. Particularly interesting were the discussions after some glasses of good, greek wine. They resulted in less high-grade feedback, but much fun instead :)&lt;/p&gt;

&lt;p&gt;Thus, we look forward to attend more conferences. Well, the next conference is already set: In two weeks, three of the Hypriot team (Dieter, Stefan and myself) will attend DockerCon in Seattle, USA! We will give workshops and talks, so we are happy to meet you there! For everyone who won&amp;rsquo;t attend: Stay tuned, we try to share as much as possible via our channels.&lt;/p&gt;

&lt;p&gt;Last but not least, I give special thanks Marcel Großmann (research assistant at University of Bamberg) for his great support. He is not only providing high quality feedback to our work, meanwhile he also contributes to the HCL. Not to mention that he also provides the venue for our Docker Meetups. His continunous support is worth a virtual applause!&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;@MathiasRenner&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>