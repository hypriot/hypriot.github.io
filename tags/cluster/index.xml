<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cluster on Docker Pirates ARMed with explosive stuff</title>
    <link>https://blog.hypriot.com/tags/cluster/index.xml</link>
    <description>Recent content in Cluster on Docker Pirates ARMed with explosive stuff</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://blog.hypriot.com/tags/cluster/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Setup Kubernetes on a Raspberry Pi Cluster easily the official way!</title>
      <link>https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/</link>
      <pubDate>Wed, 11 Jan 2017 14:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; shares the pole position with Docker in the category &amp;ldquo;orchestration solutions for Raspberry Pi cluster&amp;rdquo;.
However it&amp;rsquo;s setup process has been elaborate – until &lt;a href=&#34;http://blog.kubernetes.io/2016/09/how-we-made-kubernetes-easy-to-install.html&#34;&gt;v1.4 with the kubeadm announcement&lt;/a&gt;.
With that effort, Kubernetes changed this game completely and can be up and running officially within no time.&lt;/p&gt;

&lt;p&gt;I am very happy to announce that this blog post has been written in collaboration with &lt;a href=&#34;https://github.com/luxas&#34;&gt;Lucas Käldström&lt;/a&gt;, an independent maintainer of Kubernetes (his story is very interesting,  you can read it in a &lt;a href=&#34;https://www.cncf.io/blog/2016/11/29/diversity-scholarship-series-programming-journey-becoming-kubernetes-maintainer&#34;&gt;CNCF blogpost&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/raspberry-pi-cluster.png&#34; alt=&#34;SwarmClusterHA&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;why-kubernetes&#34;&gt;Why Kubernetes?&lt;/h2&gt;

&lt;p&gt;As shown in my recent &lt;a href=&#34;http://www.slideshare.net/MathiasRenner/high-availability-performance-of-kubernetes-and-docker-swarm-on-a-raspberry-pi-cluster/&#34;&gt;talk&lt;/a&gt;, there are many software suites available to manage a cluster of computers. There is Kubernetes, Docker Swarm, Mesos, OpenStack, Hadoop YARN, Nomad&amp;hellip; just to name a few.&lt;/p&gt;

&lt;p&gt;However, at Hypriot we have always been in love with tiny devices. So when working with an orchestrator, the maximum power we wanna use is what&amp;rsquo;s provided by a Raspberry Pi. Why? We have IoT networks in mind that will hold a large share in tomorrow&amp;rsquo;s IT infrastructure. At their edges, the power required for large orchestrators simply is not available.&lt;/p&gt;

&lt;p&gt;This boundary of resources leads to several requirements that need to be checked before we start getting our hands dirty with an orchestrator:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lightweight:&lt;/strong&gt; Software should be fit on a Raspberry Pi or smaller. As proofed in my talk mentioned above, Kubernetes painlessly runs on a Raspberry Pi.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ARM compatible:&lt;/strong&gt; Since the ARM CPU architecture is designed for low energy consumption but still able to deliver a decent portion of power, the Raspberry Pi runs an ARM CPU. Thanks to Lucas, Kubernetes is ARM compatible.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;General purpose:&lt;/strong&gt; Hadoop or Apache Spark are great for data analysis. But what if your use case changes? We prefer general purpose software that allows to run &lt;strong&gt;anything&lt;/strong&gt;. Kubernetes uses a container runtime (with Docker as the 100% supported runtime for the time being) that allows to run whatever you want.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Production ready:&lt;/strong&gt; Since we compare Kubernetes against a production ready Docker suite, let&amp;rsquo;s be fair and only choose equivalents. Kubernetes itself is production ready, and while the ARM port has some small issues, it&amp;rsquo;s working exactly as expected when going the official kubeadm route, which also will mature with time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, Kubernetes seems to be a compelling competitor to Docker Swarm. Let&amp;rsquo;s get our hands on it!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;wait-what-about-kubernetes-on-arm&#34;&gt;Wait – what about &lt;em&gt;Kubernetes-on-ARM&lt;/em&gt;?&lt;/h2&gt;

&lt;p&gt;If you followed the discussion of Kubernetes on ARM for some time, you probably know about Lucas&amp;rsquo; project &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;kubernetes-on-ARM&lt;/a&gt;. Since the beginning of the movement to bring Kubernetes on ARM in 2015, this project has always been the most stable and updated.&lt;/p&gt;

&lt;p&gt;However, during 2016, Lucas&amp;rsquo; contributions have successfully been merged into official Kubernetes repositories, such that there is no point any more for using the kubernetes-on-ARM project.
In fact, the features of that project are far behind of what&amp;rsquo;s now implemented in the official repos, &lt;strong&gt;and that has been Lucas&amp;rsquo; goal from the beginning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re up to using Kubernetes, please stick to the official repos now. And as of the kubeadm documentation, &lt;strong&gt;the following setup is considered official for Kubernetes on ARM.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;at-first-flash-hypriotos-on-your-sd-cards&#34;&gt;At first: Flash HypriotOS on your SD cards&lt;/h2&gt;

&lt;p&gt;As hardware, take at least two Raspberry Pis and make sure they are connected to each other and to the Internet.&lt;/p&gt;

&lt;p&gt;First, we need an operating system. Download and flash &lt;a href=&#34;https://github.com/hypriot/image-builder-rpi/releases&#34;&gt;HypriotOS&lt;/a&gt;. The fastest way to download and flash HypriotOS on your SD cards is by using our &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;flash tool&lt;/a&gt; like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;flash --hostname node01 https://github.com/hypriot/image-builder-rpi/releases/download/v1.4.0/hypriotos-rpi-v1.4.0.img.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Provision all Raspberry Pis you have like this and boot them up.&lt;/p&gt;

&lt;p&gt;Afterwards, SSH into the Raspberry Pis with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh pirate@node01.local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The password &lt;code&gt;hypriot&lt;/code&gt; will grant you access.&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;install-kubernetes&#34;&gt;Install Kubernetes&lt;/h2&gt;

&lt;p&gt;The installation requires root privileges. Retrieve them by&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo su -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To install Kubernetes and its dependencies, only some commands are required.
First, trust the kubernetes APT key and add the official APT Kubernetes repository on every node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
$ echo &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot; &amp;gt; /etc/apt/sources.list.d/kubernetes.list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and then just install &lt;code&gt;kubeadm&lt;/code&gt; on every node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ apt-get update &amp;amp;&amp;amp; apt-get install -y kubeadm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After the previous command has been finished, initialize Kubernetes on the &lt;strong&gt;master node&lt;/strong&gt; with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm init --pod-network-cidr 10.244.0.0/16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is important that you add the &lt;code&gt;--pod-network-cidr&lt;/code&gt; command as given here, because we will use &lt;a href=&#34;https://coreos.com/flannel/docs/latest/&#34;&gt;flannel&lt;/a&gt;. Read the next notes about flannel if you wanna know why.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Some notes about flannel&lt;/strong&gt;: We picked flannel here because that’s the only available solution for ARM at the moment (this is subject to change in the future though).&lt;/p&gt;

&lt;p&gt;flannel can use and is using in this example the Kubernetes API to store metadata about the Pod CIDR allocations, and therefore we need to tell &lt;em&gt;Kubernetes&lt;/em&gt; first which subnet we want to use. The subnet we chose here is somehow fixed, because the &lt;a href=&#34;https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml#L25&#34;&gt;flannel configuration file&lt;/a&gt; that we&amp;rsquo;ll use later in this guide predefines the equivalent subnet. Of course, you can adapt both.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you are connected via WIFI instead of Ethernet, add &lt;code&gt;--apiserver-advertise-address=&amp;lt;wifi-ip-address&amp;gt;&lt;/code&gt; as parameter to &lt;code&gt;kubeadm init&lt;/code&gt; in order to publish Kubernetes&amp;rsquo; API via WiFi. Feel free to explore the other options that exist for &lt;code&gt;kubeadm init&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;After Kubernetes has been initialized, the last lines of your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/init.png&#34; alt=&#34;init&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To start using your cluster, you need to run (as a regular user):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo cp /etc/kubernetes/admin.conf $HOME/
$ sudo chown $(id -u):$(id -g) $HOME/admin.conf
$ export KUBECONFIG=$HOME/admin.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, as told by that output, let all other nodes join the cluster via the given &lt;code&gt;kubeadm join&lt;/code&gt; command. It will look something like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm join --token=bb14ca.e8bbbedf40c58788 192.168.0.34
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After some seconds, you should see all nodes in your cluster when executing the following &lt;strong&gt;on the master node&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/get-nodes.png&#34; alt=&#34;k8S&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, &lt;strong&gt;we need to setup flannel v0.9.1 as the Pod network driver&lt;/strong&gt;. Do not use &lt;a href=&#34;https://github.com/coreos/flannel/releases/tag/v0.8.0&#34;&gt;v0.8.0&lt;/a&gt; due to a known &lt;a href=&#34;https://github.com/coreos/flannel/issues/773&#34;&gt;bug&lt;/a&gt; that can cause a &lt;code&gt;CrashLoopBackOff&lt;/code&gt; error. Run this &lt;strong&gt;on the master node&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -sSL https://rawgit.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml | sed &amp;quot;s/amd64/arm/g&amp;quot; | kubectl create -f -
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your terminal should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/flannel.png&#34; alt=&#34;k8S&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then wait until all flannel and all other cluster-internal Pods are &lt;code&gt;Running&lt;/code&gt; before you continue:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get po --all-namespaces
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Nice, it seems like they are all &lt;code&gt;Running&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/show-namespaces.png&#34; alt=&#34;show-namespaces&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for the setup of Kubernetes! Next, let&amp;rsquo;s actually spin up a service on the cluster!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;test-your-setup-with-a-tiny-service&#34;&gt;Test your setup with a tiny service&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start a simple service so see if the cluster actually can publish a service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run hypriot --image=hypriot/rpi-busybox-httpd --replicas=3 --port=80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command starts set of containers called &lt;strong&gt;hypriot&lt;/strong&gt; from the image &lt;strong&gt;hypriot/rpi-busybox-httpd&lt;/strong&gt; and defines the port the container listens on at &lt;strong&gt;80&lt;/strong&gt;. The service will be &lt;strong&gt;replicated with 3 containers&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Next, expose the Pods in the above created Deployment in a Service with a stable name and IP:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment hypriot --port 80
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Great! Now, let&amp;rsquo;s check if all three desired containers are up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get endpoints hypriot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see three endpoints (= containers) like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/show-endpoints.png&#34; alt=&#34;show-endpoints&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s curl one of them to see if the service is up:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/curl-service.png&#34; alt=&#34;curl-service&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The HTML is the response of the service. Good, it&amp;rsquo;s up and running! Next, let&amp;rsquo;s see how we can access it from outside the cluster!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;finally-access-your-service-from-outside-the-cluster&#34;&gt;Finally access your service from outside the cluster&lt;/h2&gt;

&lt;p&gt;We will now deploy an example Ingress Controller to manage incoming requests from the outside world onto our tiny service. Also, in this example we we&amp;rsquo;ll use &lt;a href=&#34;https://traefik.io&#34;&gt;Traefik&lt;/a&gt; as load balancer. Read the following notes if you wanna know more about Ingress and Traefik.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;In contrast to Docker Swarm, Kubernetes itself does not provide an option to define a specific port that you can use to access a service&lt;/strong&gt;. According to Lucas this is an important design decision; routing of incoming requests should be handled by a third party, such as a load balancer or a webserver, but not by the core product. The core Kubernetes should be lean and extensible, and encourage others to build tools on top of it for their specific needs.&lt;/p&gt;

&lt;p&gt;Regarding load balancers in front of a cluster, there is &lt;a href=&#34;http://kubernetes.io/docs/user-guide/ingress/&#34;&gt;the Ingress API object&lt;/a&gt; and some sample &lt;a href=&#34;https://github.com/kubernetes/ingress&#34;&gt;Ingress Controllers&lt;/a&gt;. Ingress is a built-in way of exposing Services to the outside world via an Ingress Controller that anyone can build. An &lt;em&gt;Ingress rule&lt;/em&gt; defines how traffic should flow from the node the Ingress controller runs on to services inside of the cluster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, let&amp;rsquo;s deploy traefik as load balancer:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://raw.githubusercontent.com/hypriot/rpi-traefik/master/traefik-k8s-example.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Label the node you want to be the load balancer. Then the Traefik Ingress Controller will land on the node you specified. Run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl label node &amp;lt;load balancer-node&amp;gt; nginx-controller=traefik
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, create an Ingress object that makes Traefik load balance traffic on port &lt;code&gt;80&lt;/code&gt; to the &lt;code&gt;hypriot&lt;/code&gt; service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat &amp;gt; hypriot-ingress.yaml &amp;lt;&amp;lt;EOF
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hypriot
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: hypriot
          servicePort: 80
EOF
$ kubectl apply -f hypriot-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Visit the loadbalancing node&amp;rsquo;s IP address in your browser and you should see a nice web page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/kubernetes-setup-cluster/hypriot-website.png&#34; alt=&#34;curl-service&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t see a website there yet, run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and make sure all hypriot Pods are in the &lt;code&gt;Running&lt;/code&gt; state.&lt;/p&gt;

&lt;p&gt;Wait until you see that all Pods are running, and a nice Hypriot website should appear!&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;tear-down-the-cluster&#34;&gt;Tear down the cluster&lt;/h2&gt;

&lt;p&gt;If you wanna reset the whole cluster to the state after a fresh install, just run this on each node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubeadm reset
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In addition, it is recommended to delete some additional files &lt;a href=&#34;http://stackoverflow.com/questions/41359224/kubernetes-failed-to-setup-network-for-pod-after-executed-kubeadm-reset/41372829#41372829&#34;&gt;as it is mentioned here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;optional-deploy-the-kubernetes-dashboard&#34;&gt;Optional: Deploy the Kubernetes dashboard&lt;/h2&gt;

&lt;p&gt;The dashboard is a wonderful interface to visualize the state of the cluster. Start it with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard-arm.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Edit the kubernetes-dashboard service to use &lt;code&gt;type: ClusterIP&lt;/code&gt; to &lt;code&gt;type: NodePort&lt;/code&gt;, see &lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above&#34;&gt;Accessing Kubernetes Dashboard&lt;/a&gt; for more details.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n kube-system edit service kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command provides the port that the dashboard is exposed at on every node with the NodePort function of Services, which is another way to expose your Services to the outside of your cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n kube-system get service kubernetes-dashboard -o template --template=&amp;quot;{{ (index .spec.ports 0).nodePort }}&amp;quot; | xargs echo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you can checkout the dashboard on any node&amp;rsquo;s IP address on that port! Make sure to use &lt;code&gt;https&lt;/code&gt; when accessing the dashboard, for example if running on port &lt;code&gt;31657&lt;/code&gt; access it at &lt;code&gt;https://node:31657&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Newer versions of the Kubernetes Dashboard require either a &lt;code&gt;Kubeconfig&lt;/code&gt; or &lt;code&gt;Token&lt;/code&gt; to view information on the dashboard. &lt;a href=&#34;https://github.com/kubernetes/dashboard/wiki/Access-control#introduction&#34;&gt;Bearer tokens&lt;/a&gt; are recommended to setup proper permissions for a user, but to test the &lt;code&gt;replicaset-controller-token&lt;/code&gt; Token may be used to test.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n kube-system describe secret `kubectl -n kube-system get secret | grep replicaset-controller-token | awk &#39;{print $1}&#39;` | grep token: | awk &#39;{print $2}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/br&gt;&lt;/p&gt;

&lt;h2 id=&#34;you-like-a-follow-up&#34;&gt;You like a follow-up?&lt;/h2&gt;

&lt;p&gt;It was our goal to show that Kubernetes indeed works well on ARM (and ARM 64-bit!). For more examples including the AMD64 platform, check out the &lt;a href=&#34;http://kubernetes.io/docs/getting-started-guides/kubeadm/&#34;&gt;official kubeadm documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We might follow-up this blog post with a more in-depth post about the current and planned state of Kubernetes officially on ARM and more, so stay tuned and tell Lucas if that&amp;rsquo;s something you&amp;rsquo;re interested in reading.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;Mathias Renner&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/kubernetesonarm&#34;&gt;Lucas Käldström&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Testing High Availability of Docker Swarm on a Raspberry Pi Cluster (Updated)</title>
      <link>https://blog.hypriot.com/post/high-availability-with-docker/</link>
      <pubDate>Wed, 26 Oct 2016 18:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/high-availability-with-docker/</guid>
      <description>&lt;p&gt;In its release in June this year, Docker announced two exciting news about the Docker Engine: First, the Engine 1.12 comes with built-in high availability features, called &amp;ldquo;Docker Swarm Mode&amp;rdquo;. And second, Docker started providing official support for the ARM architecture.&lt;/p&gt;

&lt;p&gt;These two news combined beg for testing the new capabilities in reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/high-availability-docker-swarm.png&#34; alt=&#34;SwarmClusterHA&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;old&amp;rdquo; Docker Swarm already offered some high availability features, but the new called &amp;ldquo;Swarm Mode&amp;rdquo; goes far beyond: To me, the most important capabilities to mention are: built-in security, rolling updates and - maybe one of the most important benefits - super user-friendliness. Production ready clustering based on powerful container technology has probably never been easier before.&lt;/p&gt;

&lt;p&gt;Also, the release of Docker 1.12 comes with official support for the ARM architecture. Running Docker on ARM has been possible for long time, but now, users can install it the same way on their Raspberry Pis as they do it on any other architecture: Using Docker&amp;rsquo;s official repositories. If you are one of our regular readers, you know that our team at Hypriot intensively collaborated with Docker to make this official support possible. So we are also very happy that this huge milestone that many people asked for is eventually checked!&lt;/p&gt;

&lt;p&gt;Now, with these two news, let&amp;rsquo;s get our hands dirty and test if the announced promises hold.&lt;/p&gt;

&lt;h2 id=&#34;setup-the-cluster&#34;&gt;Setup the cluster&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;As hardware&lt;/strong&gt;, I have 5x Raspberry Pi 3, connected in a network with Internet access.&lt;/p&gt;

&lt;p&gt;The Raspberry Pis with their SD cards, the network switch, power input and all cables live in a &lt;a href=&#34;https://www.picocluster.com/collections/starter-picocluster-kits/products/pico-5-raspberry-pi-starter-kit?variant=29344698892&#34;&gt;hardware kit from PicoCluster&lt;/a&gt;. Thanks to PicoCluster for providing the kit to us for testing!&lt;/p&gt;

&lt;p&gt;See how the cluster looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/PicoCluster.jpg&#34; alt=&#34;PicoClusterBuilt&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;As software&lt;/strong&gt;, I chose HypriotOS. I also tried Raspbian, but it requires a large system upgrade after the first boot in order to provide all features required by Docker (e.g. VXLAN kernel module). Before and after the system upgrade, I also had to run &lt;a href=&#34;https://github.com/docker/docker/blob/master/contrib/check-config.sh&#34;&gt;Docker&amp;rsquo;s test script&lt;/a&gt; to make sure everything is ok. All of this is not a big deal, but if there is an OS available that comes out of the box with all that Docker needs (and even adds lots of useful optimizations), it simply saves time and provides a hassle-free experience. So I flashed all SD cards with HypriotOS using our &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;flash tool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To connect to the cluster&lt;/strong&gt;, I SSHed into all the nodes via &lt;a href=&#34;http://gnometerminator.blogspot.de/p/introduction.html&#34;&gt;Terminator&lt;/a&gt;. Terminator is my preferred tool to organize multiple terminals in a single window. It allows to send commands to several terminals at once. Soon, I&amp;rsquo;ll have a look at tmux and might switch, but for now I recommend Terminator :)&lt;/p&gt;

&lt;p&gt;Finally, we have 5 terminals opened, all waiting for commands as you see in the following image. Please don&amp;rsquo;t care about the commands running in each terminal for now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/high-availability-testing/terminal.png&#34; alt=&#34;Terminator&#34; /&gt;&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all to do before getting our hands on Docker itself!&lt;/p&gt;

&lt;h2 id=&#34;setup-docker-swarm-on-the-cluster&#34;&gt;Setup Docker Swarm on the cluster&lt;/h2&gt;

&lt;p&gt;On HypriotOS, you&amp;rsquo;ll have the latest Docker installed, so we can instantly start playing.&lt;/p&gt;

&lt;p&gt;On an arbitrary node of the cluster, run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker swarm init
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output should look similar to that:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Swarm initialized: current node (node-master) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join \
    --token SWMTKN-1-1acm9qa7b0hmzz5v8t40d75v5fsgckeu2z5ds6ls0x7cny7l8p-307wqrc8756akpxjxls9abbbs \
    [2a02:810d:8600:2a78:f6c6:d67d:6912:17ca]:2377

To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions.

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As explained in this output, now you need to run the presented &lt;code&gt;docker swarm join ...&lt;/code&gt; command on all other nodes, which you want to join the cluster.&lt;/p&gt;

&lt;p&gt;Finally, on the node at which you executed &lt;code&gt;docker swarm init&lt;/code&gt;, check if all nodes of your cluster successfully formed a swarm:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker node ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the terminal prints a list of all nodes of your cluster, the setup is all done! Forming a cluster cannot be easier, can it?&lt;/p&gt;

&lt;h2 id=&#34;execution-of-tests&#34;&gt;Execution of tests&lt;/h2&gt;

&lt;p&gt;My tests of Docker&amp;rsquo;s ability to recover from failures comprise the following use cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ethernet interface of a random node (master or slave) got unavailable&lt;/li&gt;
&lt;li&gt;A random node completely rebooted&lt;/li&gt;
&lt;li&gt;A random node instantly crashed (unplug power source)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I documented one of the test runs in the following screencast.&lt;/p&gt;

&lt;div align=&#34;center&#34;&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/185361173&#34; width=&#34;640&#34; height=&#34;360&#34; frameborder=&#34;0&#34; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&#34;https://vimeo.com/185361173&#34;&gt;Testing High Availability Docker Swarm Mode&lt;/a&gt; from &lt;a href=&#34;https://vimeo.com/user54109827&#34;&gt;Mathias Renner&lt;/a&gt; on &lt;a href=&#34;https://vimeo.com&#34;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;As shown in the screencast, Docker is able to recover from failure of the ethernet interface. After testing the other use cases later on, Docker recovered flawlessly from a reboot and crash as well. Note that this also holds for master nodes, not only for slave nodes! This is remarkable because in contrast to slave nodes, master nodes hold important data of the cluster state. A master nodes&amp;rsquo;s outage is critical for the health of the cluster. With e.g. &lt;a href=&#34;https://github.com/luxas/kubernetes-on-arm&#34;&gt;Kubernetes-on-arm&lt;/a&gt;, the cluster does not recover after rebooting or crashing a master node.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Edit on 6.11.2016&lt;/strong&gt;: Jérémy Derussé and Nikolay Kushin reported in the discussion below that for them a reboot or crash of a node did not result in a healthy cluster state after bringing up the node again. As of today, unfortunately I can confirm this, with Docker version 1.12.1, 1.12.2 as well as 1.12.3. I cannot explain why during my tests for this post a crashed node recovered smoothly.&lt;/p&gt;

&lt;p&gt;Based on Jeremy&amp;rsquo;s report, I was able to create a quick fix for this issue. On a node, simply run:&lt;/p&gt;

&lt;p&gt;&amp;ldquo;sudo crontab -e&amp;rdquo;&lt;/p&gt;

&lt;p&gt;There, insert the following line (without quotation marks) and save the file.&lt;/p&gt;

&lt;p&gt;&amp;rdquo;@reboot docker ps&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Having this configured, after a crash or reboot the node recovered correctly in my tests. I look forward for feedback about this issue! Please use the discussion below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;more-to-come-soon&#34;&gt;More to come soon&lt;/h2&gt;

&lt;p&gt;This post is only a small chunk of the data I gathered during the tests. The next option to get the details is during my talk at the &lt;a href=&#34;http://highload.co/&#34;&gt;HighLoad++ Conference&lt;/a&gt; in Moskow, Russia. I&amp;rsquo;d be happy if you can make it there!&lt;/p&gt;

&lt;p&gt;Also, this is not the end of the story of course. Thorough testing requires also measuring incoming requests from an external load tester to the cluster while a failure occurs. After some research, I have not found any evidence that someone has ever performed that tests (please correct me if I&amp;rsquo;m wrong!).&lt;/p&gt;

&lt;p&gt;Moreover, Kubernetes received much attention lately since its support for ARM became better and better. So wouldn&amp;rsquo;t it be interesting to see if Docker or Kubernetes is better in keeping a service available and performing well, even if there are outages in the cluster?&lt;/p&gt;

&lt;p&gt;So there&amp;rsquo;s more coming soon, just stay tuned for the upcoming posts :)&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;@MathiasRenner&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Learning about Cloud Computing with Hypriot Cluster Lab at ICT 2016</title>
      <link>https://blog.hypriot.com/post/talk_ict_2016_international_conference_telecommunications/</link>
      <pubDate>Mon, 06 Jun 2016 18:03:34 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/talk_ict_2016_international_conference_telecommunications/</guid>
      <description>&lt;p&gt;Our cooperation with University of Bamberg pays off once again: We had the chance to present our Hypriot Cluster Lab at &lt;a href=&#34;http://ict-2016.org/&#34;&gt;ICT 2016&lt;/a&gt;, the International Conference on Telecommunications. There, we were able to show the Hypriot Cluster Lab to the attendees, who were mainly scientific researchers from all over the globe.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/ict_logo.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;We built the Hypriot Cluster Lab as lab environment that allows to easily teach about clustering and cloud computing. At the conference, the Hypriot Cluster Lab piqued lots of interest, so we are confident that it will be used more and more in educational institutions.&lt;/p&gt;

&lt;p&gt;Now, after the show, we are publishing &lt;a href=&#34;http://ict-2016.org/pdf/Demo1.pdf&#34;&gt;the paper&lt;/a&gt; that has been accepted at the conference and the &lt;a href=&#34;https://blog.hypriot.com/images/ict-2016-greece/ICT-Presentation.pdf&#34;&gt;presentation slides&lt;/a&gt;. However, the paper is (intentionally) very brief and the slides are not throughout self-explanatory, so we&amp;rsquo;ve also written down what we&amp;rsquo;ve presented to the audience back in Greece.&lt;/p&gt;

&lt;h2 id=&#34;iot-requires-expansion-to-small&#34;&gt;IoT requires &amp;ldquo;Expansion to small&amp;rdquo;&lt;/h2&gt;

&lt;p&gt;Building cloud computing infrastructures with big servers is well understood. Even large cloud infrastructures powered by container technology are commonly in use. One of the pioneers to mention here is Google with its container-based cloud platform &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, the uprising Internet of Things (IoT) will not allow to equip the edge of the network such as sensor networks in smart home/grid/production environments with big servers only. There, you especially need small devices with less computational power and energy demand. These devices require to be small and cheap enough to install up to several thousands of them in a smart environment, thereby creating sensor networks. For example think of sensor networks on wind farms:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/wind.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;
&lt;div style=&#34;padding-left:34.4em; font-size: smaller&#34;&gt;Image courtesy of &lt;a href=&#34;https://www.flickr.com/photos/ewea/16577911487/in/photolist-rfW8w8-ipD2tM-ipC49q-7d8FqR-ipCNzR-ipC1dU-ipBRBC-ipCGWN-ipCkFm-njeuG6-ipCC31-ipCnPv-ayYLge-ipCJ61-ayYMmc-ipBGVN-ayYHLH-9mZuNi-ayYMzX-oWbzp6-ayYK6n-7ZaHBQ-q8T7wm-az2p4W-8LowiB-az2o8s-ayYJJt-az2p2f-3euhCb-ayYMWD-ayYNgZ-ayYL48-az2riu-6YiWvR-az2qkC-ayYMev-az2rxU-az2on1-ayYK2V-ayYHrz-az2oru-az2ov7-az2oeW-qQMUJN-auVRqy-az2oDm-ayYM8Z-i5kW85-ayYJTV-az2rCE&#34;&gt;European Wind Energy Association&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;At this point, Hypriot Cluster Lab (HCL) comes into play:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;HCL is a proof of concept that building clusters using container technology (in our case: Docker) also works well on small devices, not only on big servers.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This &amp;ldquo;Expansion to Small&amp;rdquo; was the theme of the ICT 2016, which made HCL a perfect fit to the conference&amp;rsquo;s agenda.&lt;/p&gt;

&lt;h2 id=&#34;hcl-makes-a-tangible-iot-like-cluster-affordable&#34;&gt;HCL makes a tangible IoT-like cluster affordable&lt;/h2&gt;

&lt;p&gt;HCL is not only a proof of concept that clustering with container technology on IoT devices works, it also serves a second purpose: It is cheap enough to be used as a learning platform to play with clustering or cloud computing on your desk. The minimal hardware configuration requires three Raspberry Pis plus accessoires, which is affordable for everyone to get their hands on a real, tangible cluster. We&amp;rsquo;ve already seen lecturers, students and pupils getting started on the HCL with different topics related to cloud computing, e.g. playing with micro services or load balancing. Some use cases can be found &lt;a href=&#34;https://github.com/hypriot/rpi-cluster-lab-demos&#34;&gt;in one of our repos here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even though the HCL is intended to be used as a tool to learn, its containing concepts are applicable for promising IoT software solutions. Comparing IoT&amp;rsquo;s general requirements for hard- and software that I identified in my bachelor thesis (&lt;a href=&#34;https://medium.com/@mathiasrenner/docker-container-virtualization-and-the-internet-of-things-bachelor-thesis-a6bc783b81fa#.f09czsq2e&#34;&gt;details here&lt;/a&gt;) show that many of those aspects are already visible in the HCL:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/ict-2016-greece/iot-requirements.jpg&#34; alt=&#34;IoT-requirements&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;All in all, the conference was an enriching experience. Discussing with scientific researches and professors from many different countries resulted in high-grade feedback and inspirations of how we can further improve the HCL. Particularly interesting were the discussions after some glasses of good, greek wine. They resulted in less high-grade feedback, but much fun instead :)&lt;/p&gt;

&lt;p&gt;Thus, we look forward to attend more conferences. Well, the next conference is already set: In two weeks, three of the Hypriot team (Dieter, Stefan and myself) will attend DockerCon in Seattle, USA! We will give workshops and talks, so we are happy to meet you there! For everyone who won&amp;rsquo;t attend: Stay tuned, we try to share as much as possible via our channels.&lt;/p&gt;

&lt;p&gt;Last but not least, I give special thanks Marcel Großmann (research assistant at University of Bamberg) for his great support. He is not only providing high quality feedback to our work, meanwhile he also contributes to the HCL. Not to mention that he also provides the venue for our Docker Meetups. His continunous support is worth a virtual applause!&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://twitter.com/MathiasRenner&#34;&gt;@MathiasRenner&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Let&#39;s build a PicoCluster for Docker Swarm</title>
      <link>https://blog.hypriot.com/post/lets-build-a-pi-docker-picocluster/</link>
      <pubDate>Wed, 23 Mar 2016 18:40:04 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/lets-build-a-pi-docker-picocluster/</guid>
      <description>&lt;p&gt;As we love to use Docker Swarm on a cluster of Raspberry Pi&amp;rsquo;s, we&amp;rsquo;d like to cover
in this hands-on tutorial how to build such a cluster easily with a hardware kit
from &lt;a href=&#34;http://picocluster.com&#34;&gt;PicoCluster&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All you need is a PicoCluster kit for a 3-node or 5-node cluster, a couple of
Raspberry Pi&amp;rsquo;s and the time to assemble all the parts together. The project can be
completed within an hour only, and makes so much fun, too - especially when you can
share this as quality time with your kids.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-pdu-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;PicoCluster had just sent us two different kits of their cool Raspberry Pi
clusters. Of course, we are eager to build these new clusters so you can get a first impression.&lt;/p&gt;

&lt;h3 id=&#34;what-s-in-the-box&#34;&gt;What&amp;rsquo;s in the box&lt;/h3&gt;

&lt;p&gt;3-node PicoCluster kit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acrylic case, all parts&lt;/li&gt;
&lt;li&gt;PDU (12V input, 4x USB 5V output), including all wires&lt;/li&gt;
&lt;li&gt;wiring for 3x microUSB power&lt;/li&gt;
&lt;li&gt;case wiring for 1x HDMI&lt;/li&gt;
&lt;li&gt;case wiring for 1x 12V power plug&lt;/li&gt;
&lt;li&gt;base mounting for Pi tower, including all crews, nuts &amp;amp; bolts, spacers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-parts.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;5-node PicoCluster kit:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;acrylic case, all parts&lt;/li&gt;
&lt;li&gt;internal 8-port Ethernet switch&lt;/li&gt;
&lt;li&gt;PDU (12V input, 5x USB 5V output), including all wires&lt;/li&gt;
&lt;li&gt;wiring for 5x microUSB power&lt;/li&gt;
&lt;li&gt;wiring for network (Raspberry Pi to Ethernet switch)&lt;/li&gt;
&lt;li&gt;case wiring for 1x Ethernet&lt;/li&gt;
&lt;li&gt;case wiring for 1x HDMI&lt;/li&gt;
&lt;li&gt;case wiring for 2x USB&lt;/li&gt;
&lt;li&gt;case wiring for 1x 12V power plug&lt;/li&gt;
&lt;li&gt;base mounting for Pi tower, including all crews, nuts &amp;amp; bolts, spacers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-parts.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not included in the kits: (so you have to buy it separately)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Raspberry Pi’s and microSD cards&lt;/li&gt;
&lt;li&gt;AC adapter (12V, 1.5A, barrel plug 5.5x2.1mm)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Additionally we&amp;rsquo;ll need some common tools for the mechanical assembling:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;philips screw driver&lt;/li&gt;
&lt;li&gt;7mm wrench (or a pliers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/tools-screwdriver-wrench.jpg&#34; alt=&#34;PicoCluster toosl&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;assembling-the-3-node-picocluster&#34;&gt;Assembling the 3-node PicoCluster&lt;/h3&gt;

&lt;h4 id=&#34;towering-the-raspberry-pi-s&#34;&gt;Towering the Raspberry Pi&amp;rsquo;s&lt;/h4&gt;

&lt;p&gt;First we take the mounting plate for the Pi tower and stack all
three Raspberry Pi&amp;rsquo;s with the help of the included spacers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-pi-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;wiring-all-together&#34;&gt;Wiring all together&lt;/h4&gt;

&lt;p&gt;Now it&amp;rsquo;s easy to mount the PDU on top of the Pi tower with 4 screws and attaching
the USB power cables to the three Raspberry Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-pdu-tower2.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Next we mount the tower on the base plate, which makes our new cluster look even better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-base-tower.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From now on, it&amp;rsquo;s easy to complete the cluster installation: We connect
the 12V power cable to the PDU and front cover. We can also attach the HDMI cable
to the upper Raspberry Pi and mount the other end to the front cover, too.&lt;/p&gt;

&lt;h4 id=&#34;assembling-the-case&#34;&gt;Assembling the case&lt;/h4&gt;

&lt;p&gt;Finally, we mount the side and back cover and put on the top plate.
Mounting with nuts &amp;amp; bolts and the PicoCluster is ready.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; Don&amp;rsquo;t forget to flash your microSD cards and insert them into the Pi&amp;rsquo;s,
because as soon as we close the cluster case, it&amp;rsquo;s a little bit harder to change
the SD cards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-completed.jpg&#34; alt=&#34;PicoCluster 3-node&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;assembling-the-5-node-picocluster&#34;&gt;Assembling the 5-node PicoCluster&lt;/h3&gt;

&lt;h4 id=&#34;towering-the-raspberry-pi-s-1&#34;&gt;Towering the Raspberry Pi&amp;rsquo;s&lt;/h4&gt;

&lt;p&gt;With the 5-node cluster we start similar as with the 3-node. We take
the mounting plate for the Pi tower and stack all five Raspberry Pi&amp;rsquo;s with the
spacers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-pi-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;
&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-pi-tower2.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;wiring-all-together-1&#34;&gt;Wiring all together&lt;/h4&gt;

&lt;p&gt;Next, we mount the PDU on top of the Pi tower with 4 screws and attach
the USB power cables to the five Raspberry Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-pdu-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As soon as we mount the tower on the base plate, we see that we do have a lot more
parts left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-base-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the 5-node cluster we have an internal dedicated 8-port switch, which we&amp;rsquo;ll
install inside the case and wire all the ethernet cables to the Pi&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-ethernet-switch.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From now on, it&amp;rsquo;s easy to complete the cluster installation: We connect
the 12V power cable to the PDU and front cover. We can also attach the HDMI cable
to the upper Raspberry Pi and mount the other end to the front cover, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-5node-switch-tower.jpg&#34; alt=&#34;PicoCluster 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll also find two USB cables in the kit, which you can connect to one or two
different Pi&amp;rsquo;s and mount the plug to the front panel as well. Attaching HDMI and USB
plugs to the case is really optional.&lt;/p&gt;

&lt;h4 id=&#34;assembling-the-case-1&#34;&gt;Assembling the case&lt;/h4&gt;

&lt;p&gt;Finally, we mount the side and back cover and put on the top plate.
Mounting with nuts &amp;amp; bolts and our second PicoCluster is almost ready. As there are a
few more parts to assemble the 5-node cluster, it will take a little bit longer than
building the smaller cluster.&lt;/p&gt;

&lt;h3 id=&#34;finally-we-have-two-new-clusters&#34;&gt;Finally, we have two new clusters&lt;/h3&gt;

&lt;p&gt;At the end we have now built two new Raspberry Pi clusters, a 3-node and a 5-node
from PicoCluster. Both are looking really neat and everything is stowed away in
a perfect way. Only accessing the microSD cards is not optimal once after the case
is closed. You just have to remove only a few screws and dismounting the rear side
panel of the case and then you can easily access the SD card slots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/picocluster-kits/picocluster-3node-and-5node-cluster.jpg&#34; alt=&#34;PicoCluster 3- and 5-node&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Because the 5-node cluster includes an integrated 8-port Ethernet switch, we&amp;rsquo;d recommend
it to use as a standalone Pi cluster for experimenting with Docker Swarm and other
cluster related tutorials and demos. You just have to attach a 12V power source and
a single network link. This is all you need to start playing with it right away.&lt;/p&gt;

&lt;p&gt;Now, you can go ahead and install software on your new PicoCluster.
However, this isn&amp;rsquo;t within the scope of this hands-on project, so we&amp;rsquo;ll point you to one of our
latest tutorials where you learn &lt;a href=&#34;https://blog.hypriot.com/post/how-to-setup-rpi-docker-swarm/&#34;&gt;how to setup a Docker Swarm cluster with Raspberry Pi&amp;rsquo;s&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;where-can-you-get-it&#34;&gt;Where can you get it ?&lt;/h3&gt;

&lt;p&gt;You can order your own ready-to-use and tiny data center directly at &lt;a href=&#34;http://picocluster.com&#34;&gt;PicoCluster&lt;/a&gt;
with the Raspberry Pi&amp;rsquo;s included. The hardware kits we used in this post should be available soon, too.
PicoCluster is currently optimizing a few parts like the PDU to get more power for the new Raspberry Pi 3,
so we expect the next version will get some changes and improvements compared to these
beta units.&lt;/p&gt;

&lt;p&gt;Please send us your feedback on our &lt;a href=&#34;https://gitter.im/hypriot/talk&#34;&gt;Gitter channel&lt;/a&gt; or tweet your thoughts and ideas on this project at &lt;a href=&#34;https://twitter.com/HypriotTweets&#34;&gt;@HypriotTweets&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Dieter &lt;a href=&#34;https://twitter.com/Quintus23M&#34;&gt;@Quintus23M&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Setting up 100 nodes Jenkins cluster with Docker Swarm in less than 10 minutes</title>
      <link>https://blog.hypriot.com/post/setting-up-100-nodes-jenkins-cluster-with-docker-swarm-in-less-than-10-minutes/</link>
      <pubDate>Sun, 20 Mar 2016 23:00:00 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/setting-up-100-nodes-jenkins-cluster-with-docker-swarm-in-less-than-10-minutes/</guid>
      <description>&lt;p&gt;A week ago Scaleway announced their new C2 server. I was so excited to see that they were introducing new ARM C2 servers with 8 CPUs and 32 GB of RAM. Wow!
Not until I logged in and did &lt;code&gt;lscpu&lt;/code&gt; did I realize that those servers were &amp;ldquo;just&amp;rdquo; Intel Atom servers.&lt;/p&gt;

&lt;p&gt;A bit disappointed I thought what to do now? In my enthusiasm I had spun up 3 servers with 82 GB of RAM and 24 CPUs.
Still with my thoughts firmly rooted in the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-join-your-first-swarm/&#34;&gt;Docker #SwarmWeek&lt;/a&gt; I could not resist to find out how easy it would be to set up a Swarm Cluster on those servers.&lt;/p&gt;

&lt;p&gt;But a Swarm Cluster on its own is like a container ship without any containers - so I decided to run a Jenkins Cluster on top of the Swarm Cluster.&lt;/p&gt;

&lt;p&gt;You might not believe me, but I was able to set up a 100 node Jenkins Cluster in less than 10 minutes.
And you can do that, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/100-nodes-jenkins/scaleway.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;There are couple of reasons why it was so simple.&lt;/p&gt;

&lt;h2 id=&#34;meet-the-new-scaleway-c2-servers&#34;&gt;Meet the new Scaleway C2 servers&lt;/h2&gt;

&lt;p&gt;The first reason is that I was using the new Scaleway Cloud servers.&lt;/p&gt;

&lt;p&gt;You might think &amp;ldquo;well, just ordinary cloudservers like those Amazon EC2 instances&amp;rdquo;, but the Scaleway offering is really quite different.
By buying a Scaleway server you get access to real hardware that only belongs to you. It is not a virtual machine running on shared hardware.
Still you have all the flexibilty of an - say Amazon EC2 instance - you can start and stop the instance on demand and pay as you go on an hourly rate.
Furthermore you can completely control the Scaleway servers with an API and a great command line interface.&lt;/p&gt;

&lt;p&gt;Did I already talk about the pricing model?&lt;/p&gt;

&lt;p&gt;As the Scaleway offering feels quite similar to Amazons AWS EC2 offering I looked for an instance type that was more or less comparable to the top model from Scaleway.
I ended up with an &lt;a href=&#34;http://www.ec2instances.info/?selected=m4.2xlarge&#34;&gt;m4.2xlarge instance type&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;Product&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;CPU&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;RAM&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Hourly Price&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Price per Month&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Amazon EC2 m4.2xlarge&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8 vCore&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32 GB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;€ 0,5059&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;€ 364,24&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;Scaleway Baremetal C2L&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8 Core&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;32 GB&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;€ 0,0333&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;€ 23,99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you can see the EC2 instance costs roughly 15 times what the Scaleway server costs.
Even if those two offerings are not 100% comparable I find the pricing of the Scaleway server very impressive.&lt;/p&gt;

&lt;p&gt;After introducing the new Scaleway C2 server let&amp;rsquo;s get started by setting up a couple of the C2 servers.&lt;/p&gt;

&lt;p&gt;If you do not have an account with Scaleway you need to &lt;a href=&#34;https://cloud.scaleway.com/#/signup&#34;&gt;sign up&lt;/a&gt; with them first.
Afterwards you can use the &lt;a href=&#34;https://github.com/scaleway/scaleway-cli&#34;&gt;Scaleway Command Line Interface&lt;/a&gt; to create the servers.&lt;/p&gt;

&lt;p&gt;Download the latest version for your operating system from the &lt;a href=&#34;https://github.com/scaleway/scaleway-cli/releases/tag/v1.8.0&#34;&gt;release page&lt;/a&gt;.
For working with the new C2 server you need at the latest version 1.8.0 of the Scaleway CLI.&lt;/p&gt;

&lt;p&gt;For Mac user the easiest way to install it is via &lt;a href=&#34;http://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ brew install scw
==&amp;gt; Downloading https://homebrew.bintray.com/bottles/scw-1.7.1.mavericks.bottle.tar.gz
Already downloaded: /Library/Caches/Homebrew/scw-1.7.1.mavericks.bottle.tar.gz
==&amp;gt; Pouring scw-1.7.1.mavericks.bottle.tar.gz
==&amp;gt; Caveats
Bash completion has been installed to:
  /usr/local/etc/bash_completion.d

zsh completion has been installed to:
  /usr/local/share/zsh/site-functions
==&amp;gt; Summary
🍺  /usr/local/Cellar/scw/1.7.1: 4 files, 10.7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if the Scaleway CLI works with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw --version
scw version v1.7.1, build homebrew
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks good.&lt;/p&gt;

&lt;p&gt;To be able to do anything meaningful with the CLI we need first to log into our account.
Use the email and password you got when you registered as credentials.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw login
Login (cloud.scaleway.com): somemail@somewhere.com
Password:
Do you want to upload an SSH key ?
[0] I don&#39;t want to upload a key !
[3] id_rsa.pub
Which [id]:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When you are asked to upload the public part of your SSH key you should do so.
This allows us later to securely connect to our Scaleway ARM servers via SSH.&lt;/p&gt;

&lt;p&gt;After a successful login we are now able to interact with our Scaleway account remotely.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s create three servers for our Swarm Cluster.
The C2 server type is currently in preview which means there is a quota that allows only to spin up a limited number of each instance type.
So I settled for two C2L and one C2M.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw create --commercial-type=C2L --name=cl-leader Docker
16ca1b31-f94e-4d6c-be46-ade4641054e4

$ scw create --commercial-type=C2L --name=&amp;quot;cl-follower1&amp;quot; Docker
16ca1b31-f94e-4d6c-be46-ade4641052e4

$ scw create --commercial-type=C2M --name=&amp;quot;cl-follower2&amp;quot; Docker
16ca1b31-f94e-4d6c-be46-ade4641053e4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That was easy, right?&lt;/p&gt;

&lt;p&gt;We can list our current servers by using&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw ps
SERVER ID           IMAGE               COMMAND             CREATED             STATUS              PORTS               NAME
f20a3a57            Docker_1_10_0                           1 days              running             212.47.235.237      cl-leader
8cae4fd4            Docker_1_10_0                           1 days              running             163.172.135.05      cl-follower1
6al5b4e4            Docker_1_10_0                           1 days              running             163.172.135.28      cl-follower2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s check what we have by logging into one of our new servers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader

root@cl-leader:~# docker info
root@cl-leader:~# docker info
Containers: 0
 Running: 0
 Paused: 0
 Stopped: 0
Images: 0
Server Version: 1.10.3
Storage Driver: aufs
 Root Dir: /var/lib/docker/aufs
 Backing Filesystem: extfs
 Dirs: 0
 Dirperm1 Supported: true
Execution Driver: native-0.2
Logging Driver: json-file
Plugins:
 Volume: local
 Network: bridge null host
Kernel Version: 4.4.5-docker-1
Operating System: Ubuntu 15.10
OSType: linux
Architecture: x86_64
CPUs: 8
Total Memory: 31.34 GiB
Name: cl-leader
ID: GUOI:LSEY:LASQ:XACT:M7D2:DVA3:LYLV:5YGT:6M7O:2P7T:5VXX:HKMO
Labels:
 provider=scaleway
Codename:wily
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we do have up-to-date software:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ubuntu 15.10&lt;/li&gt;
&lt;li&gt;a  4.4 Linux Kernel&lt;/li&gt;
&lt;li&gt;Docker 1.10.3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Seems the Scaleway servers are a perfect place to run Docker and we have everything ready to move forward.&lt;/p&gt;

&lt;p&gt;The next step is setting up a private network between those three servers.
I am going to use a VPN solution called &lt;a href=&#34;https://www.tinc-vpn.org/&#34;&gt;tinc&lt;/a&gt; for that.
It is a really great peer-to-peer Mesh VPN solution, but basically you can use any VPN solution that works for you.
I am not going into the details of the tinc configuration now, because I am going to write about it in another blog post soon.&lt;/p&gt;

&lt;p&gt;So after configuring tinc we do have an additional network device on each server that is part of our private tinc network:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scw exec cl-leader &amp;quot; ip a | grep tun&amp;quot;
92: tun0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 500
    inet 10.0.0.2/24 scope global tun0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Make sure that you can ping all the other nodes in your private network like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-follower1 &amp;quot;ping 10.0.0.2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That we have the name of tinc network device is important for the configuration of the Hypriot Cluster Lab which we gonna use in the next section.&lt;/p&gt;

&lt;h2 id=&#34;easy-swarming-with-the-hypriot-cluster-lab&#34;&gt;Easy swarming with the Hypriot Cluster Lab&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/hypriot/cluster-lab&#34;&gt;Hypriot Cluster Lab&lt;/a&gt; is the second reason why it is so easy to set up our 100 node Jenkins cluster.
The Cluster Lab helps us to set up a Docker Swarm Cluster without any manual configuration in minutes.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s configure the Hypriot package repository and install the Hypriot Cluster Lab.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;curl -s https://packagecloud.io/install/repositories/Hypriot/Schatzkiste/script.deb.sh | sudo bash; apt-get install -y hypriot-cluster-lab&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the Cluster Lab itself can be used in different scenarios we still need to configure some settings of the Cluster Lab to make it play nice with a VPN solution:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server \&amp;quot;sed -i -e &#39;s|ENABLE_VLAN=&amp;quot;true&amp;quot;|ENABLE_VLAN=&amp;quot;false&amp;quot;|g&#39; -e &#39;s|ENABLE_DHCP=&amp;quot;true&amp;quot;|ENABLE_DHCP=&amp;quot;false&amp;quot;|g&#39; -e &#39;s|INTERFACE=&amp;quot;eth0&amp;quot;|INTERFACE=&amp;quot;tun0&amp;quot;|g&#39; /etc/cluster-lab/cluster.conf\&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are changing the Cluster Lab configuration to disable VLAN and DHCP support and we are also changing the Cluster Lab network interface to &amp;lsquo;tun0&amp;rsquo; which we created with tinc before.
Those settings ensure that the Cluster Lab uses our private tinc network for all cluster communication. That means also that all the Docker networks are created on top of the encrypted tinc network.&lt;/p&gt;

&lt;p&gt;The last thing we need to adjust is the Avahi daemon configuration. We need to adjust it for the peer-to-peer nature of the tinc network and restart the Avahi daemon:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-leader cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;sed -i &#39;s|#allow-point-to-point=no|allow-point-to-point=yes|g&#39; /etc/avahi/avahi-daemon.conf; systemctl restart avahi-daemon&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK. Now we are ready to start the Cluster Lab on the leader node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader &amp;quot;VERBOSE=true cluster-lab start&amp;quot;

Internet Connection
  [PASS]   tun0 exists
  [PASS]   tun0 has an ip address
  [PASS]   Internet is reachable
  [PASS]   DNS works

Networking
  [PASS]   tun0 exists
  [PASS]   Cluster leader is reachable
  [PASS]   tun0 has exactly one IP
  [PASS]   tun0 has no local link address
  [PASS]   Avahi process exists
  [PASS]   Avahi is using tun0

Docker
  [PASS]   Docker is running
  [PASS]   Docker is configured to use Consul as key-value store
  [PASS]   Docker is configured to listen via tcp at port 2375
  [PASS]   Docker listens on 10.0.0.2 via tcp at port 2375 (Docker-Engine)

Consul
  [PASS]   Consul Docker image exists
  [PASS]   Consul Docker container is running
  [PASS]   Consul is listening on port 8300
  [PASS]   Consul is listening on port 8301
  [PASS]   Consul is listening on port 8302
  [PASS]   Consul is listening on port 8400
  [PASS]   Consul is listening on port 8500
  [PASS]   Consul is listening on port 8600
  [PASS]   Consul API works
  [PASS]   Cluster-Node is pingable with IP 10.0.0.3
  [PASS]   Cluster-Node is pingable with IP 10.0.0.4
  [PASS]   Cluster-Node is pingable with IP 10.0.0.2
  [PASS]   No Cluster-Node is in status &#39;failed&#39;
  [PASS]   Consul is able to talk to Docker-Engine on port 7946 (Serf)

Swarm
  [PASS]   Swarm-Join Docker container is running
  [PASS]   Swarm-Manage Docker container is running
  [PASS]   Number of Swarm and Consul nodes is equal which means our cluster is healthy scw exec cl-leader &amp;quot;VERBOSE=true cluster-lab start&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the &lt;code&gt;VERBOSE=true&lt;/code&gt; environment variable we tell the Cluster Lab to print out all the self-tests when it starts up.
If you see any failed tests it is usually a good idea to run a &lt;code&gt;cluster-lab health&lt;/code&gt; command, which does just the self-tests again.
Usually those should be green now and have only failed the first time because of timing issues.&lt;/p&gt;

&lt;p&gt;If you still have problems check our &lt;a href=&#34;https://github.com/hypriot/cluster-lab#troubleshooting&#34;&gt;trouble shooting&lt;/a&gt; section of the Cluster Lab project.&lt;/p&gt;

&lt;p&gt;It is important that the leader is started up first because the other nodes need him for the self-configuration.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s start the Cluster Lab on the rest of the nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ for server in cl-follower1 cl-follower2; do
  scw exec $server &amp;quot;VERBOSE=true cluster-lab start&amp;quot;
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can now check if our Swarm Cluster is fully functional by running a &lt;code&gt;docker info&lt;/code&gt; against the Docker Swarm port:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw exec cl-leader
root@cl-leader:~# DOCKER_HOST=tcp://10.0.0.2:2378 docker info
Containers: 20
 Running: 9
 Paused: 0
 Stopped: 11
Images: 27
Role: replica
Primary: 10.0.0.3:2378
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 cl-follower1: 10.0.0.3:2375
  └ Status: Healthy
  └ Containers: 7
  └ Reserved CPUs: 0 / 8
  └ Reserved Memory: 0 B / 32.91 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-20T15:07:55Z
 cl-follower2: 10.0.0.4:2375
  └ Status: Healthy
  └ Containers: 7
  └ Reserved CPUs: 0 / 8
  └ Reserved Memory: 0 B / 16.4 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=leader, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-20T15:08:09Z
 cl-leader: 10.0.0.2:2375
  └ Status: Healthy
  └ Containers: 6
  └ Reserved CPUs: 0 / 8
  └ Reserved Memory: 0 B / 32.91 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.4.5-docker-1, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-20T15:07:52Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.4.5-docker-1
Operating System: linux
Architecture: amd64
CPUs: 24
Total Memory: 82.22 GiB
Name: 4b3fd589316c
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As the &lt;code&gt;docker info&lt;/code&gt; command now runs against the Swarm TCP port we get plenty of information about the Swarm cluster.
For instance we can see the number of nodes we have as well as the total number of CPUs and RAM we do have in the cluster: 24 CPUs and a whopping amount of 82 GB of RAM.
That&amp;rsquo;s nice isn&amp;rsquo;t it?&lt;/p&gt;

&lt;h2 id=&#34;starting-a-100-nodes-jenkins-cluster-with-docker-compose&#34;&gt;Starting a 100 Nodes Jenkins cluster with Docker-Compose&lt;/h2&gt;

&lt;p&gt;Ok, now comes the last reason why it is so fast and easy to set up 100 node Jenkins cluster.
The reason is called &lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker-Compose&lt;/a&gt; and makes it ridiculous easy to get multi-container applications up and running.&lt;/p&gt;

&lt;p&gt;To use it we need to first install it on our cluster leader node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scw exec cl-leader &amp;quot;curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose; chmod +x /usr/local/bin/docker-compose&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create a file called &amp;ldquo;docker-compose.yml&amp;rdquo; on the cluster leader with the following content:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;version: &amp;quot;2&amp;quot;

networks:
  jenkins_swarm:
    driver: overlay
    ipam:
      driver: default
      config:
        - subnet: 10.10.2.0/16
          ip_range: 10.10.2.0/24

services:
  jenkins:
    image: csanchez/jenkins-swarm
    expose:
      - 8080
      - 50000
    ports:
      - 8080:8080
    restart: always
    networks:
      - jenkins_swarm
    environment:
      - &amp;quot;constraint:NODE==cl-leader&amp;quot;

  worker:
    image: csanchez/jenkins-swarm-slave
    command: -username jenkins -password jenkins -executors 1
    networks:
      - jenkins_swarm
    environment:
      - &amp;quot;JENKINS_PORT_8080_TCP_ADDR=jenkins&amp;quot;
      - &amp;quot;JENKINS_PORT_8080_TCP_PORT=8080&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Replace the &amp;lsquo;subnet&amp;rsquo; and &amp;lsquo;ip_range&amp;rsquo; attributes in the file with configuration values that fit your private network configuration.
This is a Docker-Compose file that describes an overlay network and two services: a Jenkins Master and a Jenkins worker.
The overlay network will stretch across our Swarm Cluster and runs on top of our private network.
Because of IP adress conflicts between our private tinc network and the standard network configuration we had to add the &amp;lsquo;ipam&amp;rsquo; part.&lt;/p&gt;

&lt;p&gt;Now we are nearly finished and can start some Jenkins nodes:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@cl-leader:~# DOCKER_HOST=tcp://10.0.0.3:2378 docker-compose -p jenkins up -d
Starting jenkins_jenkins_1
Starting jenkins_worker_1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here the important part is that the docker-compose command is running against our Swarm Cluster.
&amp;lsquo;-p&amp;rsquo; option creates a project namespace and the &amp;lsquo;-d&amp;rsquo; option tell Docker-Compose to start the container in the background.
This command initally created two container: one Jenkins master node and one worker.&lt;/p&gt;

&lt;p&gt;We now need to scale the worker nodes to 99 to finally reach our goal of a 100 nodes Jenkins Cluster.&lt;/p&gt;

&lt;p&gt;And that is really easy with Docker-Compose:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@cl-leader:~# COMPOSE_HTTP_TIMEOUT=180 DOCKER_HOST=tcp://10.0.0.3:2378 docker-compose -p jenkins scale worker=20
Creating and starting 2 ...
Creating and starting 3 ...
Creating and starting 4 ...
 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If those twenty nodes are up repeat the command with 40, 60, 80 until you reach 99.
You could of course go for 99 at once but I experienced errors and timeouts so I prefer to take it slowly.&lt;/p&gt;

&lt;p&gt;To visit the GUI of the Jenkins master server we need to find out the IP address of our cluster leader node.
We can do that with the &lt;code&gt;inspect&lt;/code&gt; subcommand of the Scaleway CLI:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ scw inspect leader | grep address
   &amp;quot;address&amp;quot;: &amp;quot;212.47.225.127&amp;quot;,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So the Jenkins master can be found at: &lt;a href=&#34;http://212.47.225.127:8080&#34;&gt;http://212.47.225.127:8080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should see a page similar to&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/100-nodes-jenkins/jenkins01.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And at &lt;a href=&#34;http://212.47.225.127:8080/computer&#34;&gt;http://212.47.225.127:8080/computer&lt;/a&gt; we can find all the slave nodes, too:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/100-nodes-jenkins/jenkins02.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So to wrap it up I can say that the combination of&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the on-demand Scaleway Servers&lt;/li&gt;
&lt;li&gt;the Hypriot Cluster Lab&lt;/li&gt;
&lt;li&gt;Docker-Compose&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;allowed me to get the Jenkins Cluster running in a really short time.&lt;/p&gt;

&lt;p&gt;It might not be 10 minutes for you - especially if you first need to set up the Scaleway account and the CLI tool. But if everything is in place it can be done in 5 to 10 minutes.&lt;/p&gt;

&lt;p&gt;Hope you had some fun following along&amp;hellip;.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;Govinda &lt;a href=&#34;https://twitter.com/_beagile_&#34;&gt;@_beagile__&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Exciting Docker Swarm experiments in the Hypriot Cluster Lab</title>
      <link>https://blog.hypriot.com/post/exciting-docker-swarm-experiments-in-the-hypriot-cluster-lab/</link>
      <pubDate>Thu, 10 Mar 2016 23:00:00 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/exciting-docker-swarm-experiments-in-the-hypriot-cluster-lab/</guid>
      <description>&lt;p&gt;This post is - like the &lt;a href=&#34;https://blog.hypriot.com/post/how-to-setup-rpi-docker-swarm/&#34;&gt;last one&lt;/a&gt; - dedicated to the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-join-your-first-swarm/&#34;&gt;Docker #SwarmWeek&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you thought that it was pretty easy to set up a Swarm Cluster after our last post - you are going to be pretty surprised to find out that there is still room for improvement.&lt;/p&gt;

&lt;p&gt;With our &lt;a href=&#34;https://github.com/hypriot/cluster-lab&#34;&gt;Hypriot Cluster Lab&lt;/a&gt; it is possible to get up a Swarm Cluster with just one command.
Follow along to learn how this is possible.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://blog.hypriot.com/images/swarm-cluster-lab/cluster_lab.jpg&#34; alt=&#34;Docker Swarm&#34; /&gt;
&lt;div style=&#34;text-align:right; font-size: smaller&#34;&gt;Image courtesy of &lt;a href=&#34;https://www.flickr.com/photos/snre/10579433974/in/photolist-h7Sn93-4A4CPY-cr7gZ3-ySfs-Q2aKL-Q2GvM-akjGfr-6cX1D7-7Z4GYE-8XKoBo-82k2PZ-5J8yhX-7Sct35-h7SfKJ-h7SmSb-h7S44X-5J8y5a-4Y8HsH-p1Sz3h-4A4wom-4Bw54x-gQ23WX-iKWTQA-8D8m59-6PgZhg-8tKzAS-4kvY6K-9kaX8z-54x1BV-5xh7Sb-bLXPYZ-4zQ8mN-7KjCAt-MSdyd-qAw4Hw-awH1nc-4tKyxG-5wWtGU-4LMU7v-7NkBns-9a3pwX-h7Tuik-iAjqa-rvTgXa-h7S5uT-h7Shc1-qL7ofi-a18dLe-5RVXPz-8gLV&#34;&gt;SNRE Labs&lt;/a&gt;&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;The Hypriot Cluster Lab has already been there for while as an easy way to set up a Swarm Cluster on Raspberry Pi&amp;rsquo;s.
The basic idea was to have a SD card image that can be used to boot up a couple of Pi&amp;rsquo;s who then automatically form a Swarm Cluster.
As with any Lab the Cluster Lab provides a place to run all kind of experiments and gain new insights.&lt;/p&gt;

&lt;p&gt;As part of our recent refactoring efforts to make the Cluster Lab more reliable we have been searching for ways to test the Cluster Lab as a whole.
The idea we came up was to use &lt;a href=&#34;https://www.vagrantup.com/&#34;&gt;Vagrant&lt;/a&gt; for our integration testing efforts.&lt;/p&gt;

&lt;p&gt;Vagrant is a tool that makes it really easy to manage Virtual Machines. It allows us to describe a multi-machine setup in an easy to read text-based configuration file:&lt;/p&gt;

&lt;script src=&#34;https://gist-it.appspot.com/github/hypriot/cluster-lab/raw/master/vagrant/Vagrantfile?slice=14:40&amp;footer=minimal&#34;&gt;&lt;/script&gt;

&lt;p&gt;One of the big advantages of Vagrant is that it is available for Mac, Linux and Windows users. This means that you can run the Cluster Lab on all those operating systems, too.&lt;/p&gt;

&lt;p&gt;To get started with the Hypriot Cluster Lab you need to ensure that you have a recent version of Vagrant installed.
If not you can download it &lt;a href=&#34;https://www.vagrantup.com/downloads.html&#34;&gt;here&lt;/a&gt;. Under the hood Vagrant uses a so-called provider to run the virtual machines.
For the Cluster Lab the recommended way is to use &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox&lt;/a&gt;. Make sure it is already there or download and install it.&lt;/p&gt;

&lt;p&gt;The next step is to clone the Hypriot Cluster Lab repository like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/hypriot/cluster-lab.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Afterwards change into the &lt;code&gt;cluster-lab/vagrant&lt;/code&gt; folder:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd cluster-lab/vagrant
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now comes the single command you need to boot up a whole working Swarm Cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant up --provider virtualbox --no-color
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command will create three virtual machines based on Ubuntu Wily, install a recent Docker Engine and install and configure the Cluster Lab.
As this command will download Ubuntu Wily and lots of other software, too, it might take while. Please be patient.&lt;/p&gt;

&lt;p&gt;While the Cluster Lab is booting it will run a number of self-tests to ensure that it is working properly.
You should see output similar to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Internet Connection
  [PASS]   eth1 exists
  [PASS]   eth1 has an ip address
  [PASS]   Internet is reachable
  [PASS]   DNS works

Networking
  [PASS]   eth1 exists
  [PASS]   vlan os package exists
  [PASS]   Avahi os package exists
  [PASS]   Avahi-utils os package exists
  [PASS]   Avahi process exists
  [PASS]   Avahi cluster-leader.service file is absent

Configure basic networking

Networking
  [PASS]   eth1.200 exists
  [PASS]   eth1.200 has correct IP from vlan network
  [PASS]   Cluster leader is reachable
  [PASS]   eth1.200 has exactly one IP
  [PASS]   eth1.200 has no local link address
  [PASS]   Avahi process exists
  [PASS]   Avahi is using eth1.200
  [PASS]   Avahi cluster-leader.service file exists

This node is Leader

DHCP is enabled

DNSmasq
  [PASS]   dnsmasq os package exists
  [PASS]   dnsmasq process exists
  [PASS]   /etc/dnsmasq.conf backup file is absent

Configure DNSmasq

DNSmasq
  [PASS]   dnsmasq process exists
  [PASS]   /etc/dnsmasq.conf backup file exists

Docker
  [PASS]   docker is installed
  [PASS]   Docker process exists
  [PASS]   /etc/default/docker backup file is absent

Configure Docker

Docker
  [PASS]   Docker is running
  [PASS]   Docker is configured to use Consul as key-value store
  [PASS]   Docker is configured to listen via tcp at port 2375
  [PASS]   Docker listens on 192.168.200.30 via tcp at port 2375 (Docker-Engine)

Consul
  [PASS]   Consul Docker image exists
  [PASS]   Consul Docker container is running
  [PASS]   Consul is listening on port 8300
  [PASS]   Consul is listening on port 8301
  [PASS]   Consul is listening on port 8302
  [PASS]   Consul is listening on port 8400
  [PASS]   Consul is listening on port 8500
  [PASS]   Consul is listening on port 8600
  [PASS]   Consul API works
  [PASS]   Cluster-Node is pingable with IP 192.168.200.30
  [PASS]   Cluster-Node is pingable with IP 192.168.200.45
  [PASS]   Cluster-Node is pingable with IP 192.168.200.1
  [PASS]   No Cluster-Node is in status &#39;failed&#39;
  [PASS]   Consul is able to talk to Docker-Engine on port 7946 (Serf)

Swarm
  [PASS]   Swarm-Join Docker container is running
  [PASS]   Swarm-Manage Docker container is running
  [PASS]   Number of Swarm and Consul nodes is equal which means our cluster is healthy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If everything worked all tests should be [PASS]ing.
In case that there were [FAIL]ing tests you can fix it with the help of our &lt;a href=&#34;https://github.com/hypriot/cluster-lab#troubleshooting&#34;&gt;troubleshooting&lt;/a&gt; guide.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;vagrant up&lt;/code&gt; command creates three cluster nodes called &lt;strong&gt;leader&lt;/strong&gt;, &lt;strong&gt;follower1&lt;/strong&gt; and &lt;strong&gt;follower2&lt;/strong&gt;.
To work with the Swarm Cluster we need to log into one of the cluster nodes and elevate ourself to root:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vagrant ssh leader
$ sudo su
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use the Swarm Cluster by exporting the TCP address of the Swarm Cluster with the DOCKER_HOST environment variable:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export DOCKER_HOST=tcp://192.168.200.1:3278
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now all our Docker commands will work against the Swarm Cluster instead of the local Docker Engine.&lt;/p&gt;

&lt;p&gt;Lets check if we really have a 3-Node-Swarm Cluster:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker info
Containers: 9
 Running: 9
 Paused: 0
 Stopped: 0
Images: 6
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 3
 follower1: 192.168.200.46:2375
  └ Status: Healthy
  └ Containers: 3
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.018 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-10T21:24:07Z
 follower2: 192.168.200.44:2375
  └ Status: Healthy
  └ Containers: 3
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.018 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=follower, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-10T21:24:35Z
 leader: 192.168.200.1:2375
  └ Status: Healthy
  └ Containers: 3
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 1.018 GiB
  └ Labels: executiondriver=native-0.2, hypriot.arch=x86_64, hypriot.hierarchy=leader, kernelversion=4.2.0-30-generic, operatingsystem=Ubuntu 15.10, storagedriver=overlay
  └ Error: (none)
  └ UpdatedAt: 2016-03-10T21:24:04Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.2.0-30-generic
Operating System: linux
Architecture: amd64
CPUs: 3
Total Memory: 3.054 GiB
Name: 9c2606252982
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see we really have a working Swarm Cluster running.
That was easy, was it?&lt;/p&gt;

&lt;p&gt;For more information about Docker Swarm you can follow the &lt;a href=&#34;https://blog.docker.com/2016/03/swarmweek-container-orchestration-docker-swarm/&#34;&gt;#SwarmWeek: Introduction to container orchestration with Docker Swarm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As always, use the comments below to give us feedback, discuss this post on &lt;a href=&#34;https://news.ycombinator.com/item?id=11263205&#34;&gt;HackerNews&lt;/a&gt; and share this post on Twitter, Google or Facebook.&lt;/p&gt;

&lt;p&gt;Govinda &lt;a href=&#34;https://twitter.com/_beagile_&#34;&gt;@_beagile__&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Let Docker Swarm all over your Raspberry Pi Cluster</title>
      <link>https://blog.hypriot.com/post/let-docker-swarm-all-over-your-raspberry-pi-cluster/</link>
      <pubDate>Fri, 03 Jul 2015 00:30:45 +0100</pubDate>
      
      <guid>https://blog.hypriot.com/post/let-docker-swarm-all-over-your-raspberry-pi-cluster/</guid>
      <description>&lt;p&gt;In this blog post we show you how easy it is to install Swarm on your Raspberry Pi and how to set up a Raspberry Pi Swarm cluster with the help of Docker Machine.&lt;/p&gt;

&lt;p&gt;We have built a little &amp;ldquo;Pi Tower&amp;rdquo; with three Raspberry Pi 2 model B and combined them into a Docker Swarm cluster.
&lt;/p&gt;

&lt;p&gt;As you can see in the pictures below we have mounted the three Raspberry Pi&amp;rsquo;s on top of a 5-port D-Link GBit switch. All four devices get their power from an Anker 4-port USB charger.
This makes a very solid but portable &amp;ldquo;Pi Tower&amp;rdquo; with only one power plug and one external network connector.&lt;/p&gt;

&lt;div class=&#34;gallery clearfix&#34; itemscope itemtype=&#34;http://schema.org/ImageGallery&#34;&gt;
&lt;div class=&#34;title&#34;&gt;Pi Tower&lt;/div&gt;




&lt;figure itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
  &lt;a href=&#34;https://blog.hypriot.com/images/docker-swarm/d-link_mounting_holes.jpg&#34; itemprop=&#34;contentUrl&#34; data-size=&#34;1600x1200&#34;&gt;
      &lt;img src=&#34;https://blog.hypriot.com/images/docker-swarm/thumbnails/thumb_d-link_mounting_holes.jpg&#34; itemprop=&#34;thumbnail&#34; alt=&#34;The Making of Pi Tower&#34; /&gt;
  &lt;/a&gt;


  &lt;figcaption itemprop=&#34;caption description&#34;&gt;
    The Making of Pi Tower
    &lt;span itemprop=&#34;copyrightHolder&#34;&gt;Hypriot&lt;/span&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;




&lt;figure itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
  &lt;a href=&#34;https://blog.hypriot.com/images/docker-swarm/d-link_rpi2_cluster.jpg&#34; itemprop=&#34;contentUrl&#34; data-size=&#34;1600x1200&#34;&gt;
      &lt;img src=&#34;https://blog.hypriot.com/images/docker-swarm/thumbnails/thumb_d-link_rpi2_cluster.jpg&#34; itemprop=&#34;thumbnail&#34; alt=&#34;Pi Tower with D-Link Switch&#34; /&gt;
  &lt;/a&gt;


  &lt;figcaption itemprop=&#34;caption description&#34;&gt;
    Pi Tower with D-Link Switch
    &lt;span itemprop=&#34;copyrightHolder&#34;&gt;Hypriot&lt;/span&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;




&lt;figure itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
  &lt;a href=&#34;https://blog.hypriot.com/images/docker-swarm/d-link_rpi2_cluster02.jpg&#34; itemprop=&#34;contentUrl&#34; data-size=&#34;1600x1200&#34;&gt;
      &lt;img src=&#34;https://blog.hypriot.com/images/docker-swarm/thumbnails/thumb_d-link_rpi2_cluster02.jpg&#34; itemprop=&#34;thumbnail&#34; alt=&#34;Pi Tower with D-Link Switch&#34; /&gt;
  &lt;/a&gt;


  &lt;figcaption itemprop=&#34;caption description&#34;&gt;
    Pi Tower with D-Link Switch
    &lt;span itemprop=&#34;copyrightHolder&#34;&gt;Hypriot&lt;/span&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;




&lt;figure itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
  &lt;a href=&#34;https://blog.hypriot.com/images/docker-swarm/d-link_rpi2_cluster_lights.jpg&#34; itemprop=&#34;contentUrl&#34; data-size=&#34;1600x1200&#34;&gt;
      &lt;img src=&#34;https://blog.hypriot.com/images/docker-swarm/thumbnails/thumb_d-link_rpi2_cluster_lights.jpg&#34; itemprop=&#34;thumbnail&#34; alt=&#34;Pi Tower at night&#34; /&gt;
  &lt;/a&gt;


  &lt;figcaption itemprop=&#34;caption description&#34;&gt;
    Pi Tower at night
    &lt;span itemprop=&#34;copyrightHolder&#34;&gt;Hypriot&lt;/span&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;


&lt;/div&gt;


&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.hypriot.com/css/photoswipe.css&#34;&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;https://blog.hypriot.com/css/default-skin/default-skin.css&#34;&gt;
&lt;script src=&#34;https://blog.hypriot.com/js/photoswipe.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://blog.hypriot.com/js/photoswipe-ui-default.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://blog.hypriot.com/js/initphotoswipe.js&#34;&gt;&lt;/script&gt;



&lt;div class=&#34;pswp&#34; tabindex=&#34;-1&#34; role=&#34;dialog&#34; aria-hidden=&#34;true&#34;&gt;

&lt;div class=&#34;pswp__bg&#34;&gt;&lt;/div&gt;

&lt;div class=&#34;pswp__scroll-wrap&#34;&gt;
    
    &lt;div class=&#34;pswp__container&#34;&gt;
      &lt;div class=&#34;pswp__item&#34;&gt;&lt;/div&gt;
      &lt;div class=&#34;pswp__item&#34;&gt;&lt;/div&gt;
      &lt;div class=&#34;pswp__item&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    
    &lt;div class=&#34;pswp__ui pswp__ui--hidden&#34;&gt;
    &lt;div class=&#34;pswp__top-bar&#34;&gt;
      
      &lt;div class=&#34;pswp__counter&#34;&gt;&lt;/div&gt;
      &lt;button class=&#34;pswp__button pswp__button--close&#34; title=&#34;Close (Esc)&#34;&gt;&lt;/button&gt;
      &lt;button class=&#34;pswp__button pswp__button--share&#34; title=&#34;Share&#34;&gt;&lt;/button&gt;
      &lt;button class=&#34;pswp__button pswp__button--fs&#34; title=&#34;Toggle fullscreen&#34;&gt;&lt;/button&gt;
      &lt;button class=&#34;pswp__button pswp__button--zoom&#34; title=&#34;Zoom in/out&#34;&gt;&lt;/button&gt;
      
      
      &lt;div class=&#34;pswp__preloader&#34;&gt;
        &lt;div class=&#34;pswp__preloader__icn&#34;&gt;
          &lt;div class=&#34;pswp__preloader__cut&#34;&gt;
            &lt;div class=&#34;pswp__preloader__donut&#34;&gt;&lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div class=&#34;pswp__share-modal pswp__share-modal--hidden pswp__single-tap&#34;&gt;
      &lt;div class=&#34;pswp__share-tooltip&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;button class=&#34;pswp__button pswp__button--arrow--left&#34; title=&#34;Previous (arrow left)&#34;&gt;
    &lt;/button&gt;
    &lt;button class=&#34;pswp__button pswp__button--arrow--right&#34; title=&#34;Next (arrow right)&#34;&gt;
    &lt;/button&gt;
    &lt;div class=&#34;pswp__caption&#34;&gt;
      &lt;div class=&#34;pswp__caption__center&#34;&gt;&lt;/div&gt;
    &lt;/div&gt;
    &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;


&lt;style&gt;
    .gallery {  }
    .gallery img { width: 100%; height: auto; }
    .gallery figure { display: block; float: left; margin: 0 5px 5px 0; width: 200px; }
    .gallery figcaption { display: none; }
    .gallery div.title { font-weight: bold; }
    span[itemprop=&#34;copyrightHolder&#34;] { color : #888; float: right; }
    span[itemprop=&#34;copyrightHolder&#34;]:before { content: &#34;Foto: &#34;; }
    img[itemprop=&#34;thumbnail&#34;]{ width: 200px; }
&lt;/style&gt;


&lt;script&gt;initPhotoSwipeFromDOM(&#39;.gallery&#39;);&lt;/script&gt;


&lt;p&gt;For your convenience we have prepared a &lt;a href=&#34;http://www.amazon.de/gp/registry/wishlist/BCGEW9W3V8GM/ref=cm_wl_rlist_go_o&#34;&gt;small shopping list&lt;/a&gt; of all the components we used at Amazon.&lt;/p&gt;

&lt;h2 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h2&gt;

&lt;p&gt;For this tutorial we will run all steps from a Mac. To do this we need three tools.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A flash tool to write the SD card images for all the Raspberry Pi&amp;rsquo;s.&lt;/li&gt;
&lt;li&gt;The Docker client, which is only a &lt;code&gt;brew install docker&lt;/code&gt; away.&lt;/li&gt;
&lt;li&gt;The Docker Machine binary with the hypriot driver&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;flash-all-sd-cards&#34;&gt;Flash all SD cards&lt;/h2&gt;

&lt;p&gt;First we want to install the SD cards with Docker preinstalled.
On a Mac or Linux machine we can use our little &lt;a href=&#34;https://github.com/hypriot/flash&#34;&gt;flash command line tool&lt;/a&gt; to prepare the three SD cards with these simple commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ flash --hostname pi1 http://downloads.hypriot.com/hypriot-rpi-20150416-201537.img.zip
$ flash --hostname pi2 http://downloads.hypriot.com/hypriot-rpi-20150416-201537.img.zip
$ flash --hostname pi3 http://downloads.hypriot.com/hypriot-rpi-20150416-201537.img.zip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now insert the SD cards into all Raspberry Pi&amp;rsquo;s and boot them. They will come up with different host names after a while.&lt;/p&gt;

&lt;h2 id=&#34;retrieve-ip-addresses&#34;&gt;Retrieve IP addresses&lt;/h2&gt;

&lt;p&gt;Our SD card image also starts the avahi-daemon to announce the hostname through mDNS, so each Pi is reachable with &lt;code&gt;pi1.local&lt;/code&gt;, &lt;code&gt;pi2.local&lt;/code&gt; and &lt;code&gt;pi3.local&lt;/code&gt;.
Docker Machine cannot resolve these hostnames at the moment, so we have to retrieve the IP addresses for the Raspberry Pi&amp;rsquo;s manually.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ping -c 1 pi1.local
$ ping -c 1 pi2.local
$ ping -c 1 pi3.local
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this example we assume that the three IP adresses are &lt;code&gt;192.168.1.101&lt;/code&gt;, &lt;code&gt;102&lt;/code&gt; and &lt;code&gt;103&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;insert-ssh-public-key&#34;&gt;Insert SSH public key&lt;/h2&gt;

&lt;p&gt;Docker Machine connects to each Raspberry Pi through SSH. You have to insert your public SSH key to avoid entering the password of the &lt;code&gt;root&lt;/code&gt; user.
To insert the SSH public key into a remote machine there is a tool called &lt;code&gt;ssh-copy-id&lt;/code&gt;. You might have to install it first.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ssh-copy-id root@192.168.1.101
$ ssh-copy-id root@192.168.1.102
$ ssh-copy-id root@192.168.1.103
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each of the above commands you have to enter the password &lt;code&gt;hypriot&lt;/code&gt; for the user &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;create-docker-machines&#34;&gt;Create Docker Machines&lt;/h2&gt;

&lt;p&gt;For the next step we use our Docker Machine driver to connect to the Raspberry Pi Hypriot devices.
Our hypriot driver is not yet integrated into the official Docker Machine binary.
So we have to download the &lt;code&gt;docker-machine&lt;/code&gt; binary with our hypriot machine driver.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl -o docker-machine http://downloads.hypriot.com/docker-machine_0.4.0-dev_darwin-amd64
$ chmod +x ./docker-machine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download the binary into the current directory and make it executable. You may move it
into another directory in your PATH to use it from other directories.&lt;/p&gt;

&lt;h3 id=&#34;create-swarm-token&#34;&gt;Create Swarm Token&lt;/h3&gt;

&lt;p&gt;A Docker Swarm cluster uses a unique Cluster ID which allows the individual swarm agents to find each other.
We need such a Cluster ID to build our Docker Swarm.&lt;/p&gt;

&lt;p&gt;This can be done in your shell with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export TOKEN=$(for i in $(seq 1 32); do echo -n $(echo &amp;quot;obase=16; $(($RANDOM % 16))&amp;quot; | bc); done; echo)
$ echo $TOKEN
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this example we use&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export TOKEN=babb1eb00bdecadedec0debabb1eb00b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you already have a Docker swarm container up and running, you also can create a new Cluster ID
with &lt;code&gt;docker run --rm hypriot/rpi-swarm create&lt;/code&gt;.
We simply used the shell commands above to skip this chicken or egg problem.&lt;/p&gt;

&lt;h3 id=&#34;create-the-swarm-master&#34;&gt;Create the Swarm Master&lt;/h3&gt;

&lt;p&gt;Now we create the Docker Swarm Master on the first Raspberry Pi with our generated Cluster ID&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./docker-machine create -d hypriot --swarm --swarm-master --swarm-discovery token://$TOKEN --hypriot-ip-address 192.168.1.101 pi1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This command connects to the Raspberry Pi &amp;ldquo;pi1&amp;rdquo;, secures the Docker daemon with TLS and pulls the Docker image &lt;code&gt;hypriot/rpi-swarm:latest&lt;/code&gt; from the Docker Hub.
It starts both the Swarm Master as well as a Swarm Agent in a container.&lt;/p&gt;

&lt;p&gt;To check if everything works we can connect to the newly started Swarm Master by using the following command.
It retrieves all environment variables needed for the Docker client to communicate with the Swarm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(./docker-machine env --swarm pi1)
$ docker info
Containers: 2
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 1
 pi1: 192.168.1.202:2376
  └ Containers: 2
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 971.3 MiB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now have successfully set up a lonely Swarm Manager. Let&amp;rsquo;s start some more Raspberry Pi&amp;rsquo;s to prevent the Swarm Manager from feeling lonely.&lt;/p&gt;

&lt;h3 id=&#34;create-the-swarm-agents&#34;&gt;Create the Swarm agents&lt;/h3&gt;

&lt;p&gt;For the rest of the Raspberry Pi&amp;rsquo;s we also create Docker Machine connections with the same Cluster ID.
This time we run docker-machine without the &lt;code&gt;--swarm-master&lt;/code&gt; option to just spin up a Swarm Agent container in each Pi.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./docker-machine create -d hypriot --swarm --swarm-discovery token://$TOKEN --hypriot-ip-address 192.168.1.102 pi2
$ ./docker-machine create -d hypriot --swarm --swarm-discovery token://$TOKEN --hypriot-ip-address 192.168.1.103 pi3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s check what the swarm looks like now&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker info
Containers: 4
Strategy: spread
Filters: affinity, health, constraint, port, dependency
Nodes: 3
 pi1: 192.168.1.101:2376
  └ Containers: 2
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 971.3 MiB
 pi2: 192.168.1.102:2376
  └ Containers: 1
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 971.3 MiB
 pi3: 192.168.1.103:2376
  └ Containers: 1
  └ Reserved CPUs: 0 / 4
  └ Reserved Memory: 0 B / 971.3 MiB
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also can list all containers in the whole swarm as usual with&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker ps
CONTAINER ID        IMAGE                      COMMAND                CREATED             STATUS              PORTS                                    NAMES
5effaa7de4a3        hypriot/rpi-swarm:latest   &amp;quot;/swarm join --addr    2 minutes ago       Up About a minute   2375/tcp                                 pi3/swarm-agent
6b73003b7246        hypriot/rpi-swarm:latest   &amp;quot;/swarm join --addr    4 minutes ago       Up 3 minutes        2375/tcp                                 pi2/swarm-agent
5e00fbf7b9f6        hypriot/rpi-swarm:latest   &amp;quot;/swarm join --addr    7 minutes ago       Up 7 minutes        2375/tcp                                 pi1/swarm-agent
02c905ec25a0        hypriot/rpi-swarm:latest   &amp;quot;/swarm manage --tls   7 minutes ago       Up 7 minutes        2375/tcp, 192.168.1.101:3376-&amp;gt;3376/tcp   pi1/swarm-agent-master
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After setting up the Docker Swarm you now can use the normal Docker commands through port 3376.
Have a look at the &lt;a href=&#34;https://docs.docker.com/swarm/&#34;&gt;official Docker Swarm documentation&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Just remember to set up the environment correctly to communicate with the Swarm Master before using the Docker client by&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ eval $(docker-machine env --swarm pi1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then you can remotely manage your Raspberry Pi Swarm from your Mac. See - this was not really difficult, was it?&lt;/p&gt;

&lt;p&gt;We hope you enjoyed this little tour of Docker Swarm!&lt;/p&gt;

&lt;p&gt;As always use the comments below to give us feedback and share it on Twitter or Facebook.&lt;/p&gt;

&lt;p&gt;Stefan&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>